{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "massive-librarian",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4173' max='9048' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4173/9048 21:57 < 25:39, 3.17 it/s, Epoch 3.69/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Contentrecall</th>\n",
       "      <th>Contentprecision</th>\n",
       "      <th>Contentf1</th>\n",
       "      <th>Sourcerecall</th>\n",
       "      <th>Sourceprecision</th>\n",
       "      <th>Sourcef1</th>\n",
       "      <th>Nonerecall</th>\n",
       "      <th>Noneprecision</th>\n",
       "      <th>Nonef1</th>\n",
       "      <th>Overallprecision</th>\n",
       "      <th>Overallrecall</th>\n",
       "      <th>Overallf1</th>\n",
       "      <th>Leftprecision</th>\n",
       "      <th>Leftrecall</th>\n",
       "      <th>Leftf1</th>\n",
       "      <th>Rightprecision</th>\n",
       "      <th>Rightrecall</th>\n",
       "      <th>Rightf1</th>\n",
       "      <th>Vainprecision</th>\n",
       "      <th>Vainrecall</th>\n",
       "      <th>Vainf1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.930600</td>\n",
       "      <td>1.873091</td>\n",
       "      <td>0.020153</td>\n",
       "      <td>0.041746</td>\n",
       "      <td>0.027183</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>0.084257</td>\n",
       "      <td>0.088801</td>\n",
       "      <td>0.842471</td>\n",
       "      <td>0.352793</td>\n",
       "      <td>0.497326</td>\n",
       "      <td>0.278864</td>\n",
       "      <td>0.374932</td>\n",
       "      <td>0.319839</td>\n",
       "      <td>0.327697</td>\n",
       "      <td>0.020136</td>\n",
       "      <td>0.037941</td>\n",
       "      <td>0.035675</td>\n",
       "      <td>0.021916</td>\n",
       "      <td>0.027151</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.007512</td>\n",
       "      <td>0.001557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.324500</td>\n",
       "      <td>1.264500</td>\n",
       "      <td>0.293021</td>\n",
       "      <td>0.413665</td>\n",
       "      <td>0.343044</td>\n",
       "      <td>0.033023</td>\n",
       "      <td>0.090397</td>\n",
       "      <td>0.048374</td>\n",
       "      <td>0.853510</td>\n",
       "      <td>0.374406</td>\n",
       "      <td>0.520490</td>\n",
       "      <td>0.377880</td>\n",
       "      <td>0.508059</td>\n",
       "      <td>0.433405</td>\n",
       "      <td>0.485320</td>\n",
       "      <td>0.403186</td>\n",
       "      <td>0.440456</td>\n",
       "      <td>0.020187</td>\n",
       "      <td>0.008883</td>\n",
       "      <td>0.012337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.046900</td>\n",
       "      <td>0.991093</td>\n",
       "      <td>0.540961</td>\n",
       "      <td>0.437954</td>\n",
       "      <td>0.484038</td>\n",
       "      <td>0.403510</td>\n",
       "      <td>0.327287</td>\n",
       "      <td>0.361423</td>\n",
       "      <td>0.804047</td>\n",
       "      <td>0.538317</td>\n",
       "      <td>0.644880</td>\n",
       "      <td>0.476436</td>\n",
       "      <td>0.640567</td>\n",
       "      <td>0.546442</td>\n",
       "      <td>0.494075</td>\n",
       "      <td>0.676441</td>\n",
       "      <td>0.571051</td>\n",
       "      <td>0.214352</td>\n",
       "      <td>0.216007</td>\n",
       "      <td>0.215176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>0.803029</td>\n",
       "      <td>0.670670</td>\n",
       "      <td>0.544251</td>\n",
       "      <td>0.600883</td>\n",
       "      <td>0.670095</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.494085</td>\n",
       "      <td>0.800231</td>\n",
       "      <td>0.571876</td>\n",
       "      <td>0.667051</td>\n",
       "      <td>0.539618</td>\n",
       "      <td>0.725516</td>\n",
       "      <td>0.618909</td>\n",
       "      <td>0.565164</td>\n",
       "      <td>0.739981</td>\n",
       "      <td>0.640864</td>\n",
       "      <td>0.475818</td>\n",
       "      <td>0.557805</td>\n",
       "      <td>0.513560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.734300</td>\n",
       "      <td>0.668520</td>\n",
       "      <td>0.773207</td>\n",
       "      <td>0.591527</td>\n",
       "      <td>0.670273</td>\n",
       "      <td>0.733773</td>\n",
       "      <td>0.423592</td>\n",
       "      <td>0.537117</td>\n",
       "      <td>0.787646</td>\n",
       "      <td>0.601849</td>\n",
       "      <td>0.682325</td>\n",
       "      <td>0.577108</td>\n",
       "      <td>0.775921</td>\n",
       "      <td>0.661908</td>\n",
       "      <td>0.581157</td>\n",
       "      <td>0.825625</td>\n",
       "      <td>0.682149</td>\n",
       "      <td>0.629295</td>\n",
       "      <td>0.723894</td>\n",
       "      <td>0.673287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.624928</td>\n",
       "      <td>0.817444</td>\n",
       "      <td>0.581744</td>\n",
       "      <td>0.679741</td>\n",
       "      <td>0.719498</td>\n",
       "      <td>0.502394</td>\n",
       "      <td>0.591658</td>\n",
       "      <td>0.775716</td>\n",
       "      <td>0.617366</td>\n",
       "      <td>0.687541</td>\n",
       "      <td>0.588554</td>\n",
       "      <td>0.791310</td>\n",
       "      <td>0.675035</td>\n",
       "      <td>0.565525</td>\n",
       "      <td>0.870594</td>\n",
       "      <td>0.685657</td>\n",
       "      <td>0.642872</td>\n",
       "      <td>0.771978</td>\n",
       "      <td>0.701534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.552000</td>\n",
       "      <td>0.574882</td>\n",
       "      <td>0.810935</td>\n",
       "      <td>0.615594</td>\n",
       "      <td>0.699889</td>\n",
       "      <td>0.754126</td>\n",
       "      <td>0.489346</td>\n",
       "      <td>0.593545</td>\n",
       "      <td>0.818130</td>\n",
       "      <td>0.612429</td>\n",
       "      <td>0.700490</td>\n",
       "      <td>0.601771</td>\n",
       "      <td>0.809080</td>\n",
       "      <td>0.690194</td>\n",
       "      <td>0.606418</td>\n",
       "      <td>0.851485</td>\n",
       "      <td>0.708354</td>\n",
       "      <td>0.646119</td>\n",
       "      <td>0.801624</td>\n",
       "      <td>0.715520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.592000</td>\n",
       "      <td>0.532974</td>\n",
       "      <td>0.800624</td>\n",
       "      <td>0.658515</td>\n",
       "      <td>0.722649</td>\n",
       "      <td>0.748796</td>\n",
       "      <td>0.523918</td>\n",
       "      <td>0.616490</td>\n",
       "      <td>0.870060</td>\n",
       "      <td>0.589444</td>\n",
       "      <td>0.702775</td>\n",
       "      <td>0.614036</td>\n",
       "      <td>0.825570</td>\n",
       "      <td>0.704262</td>\n",
       "      <td>0.640696</td>\n",
       "      <td>0.844875</td>\n",
       "      <td>0.728753</td>\n",
       "      <td>0.722587</td>\n",
       "      <td>0.779037</td>\n",
       "      <td>0.749751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.552800</td>\n",
       "      <td>0.513205</td>\n",
       "      <td>0.813960</td>\n",
       "      <td>0.656037</td>\n",
       "      <td>0.726515</td>\n",
       "      <td>0.759300</td>\n",
       "      <td>0.541073</td>\n",
       "      <td>0.631875</td>\n",
       "      <td>0.869841</td>\n",
       "      <td>0.598848</td>\n",
       "      <td>0.709343</td>\n",
       "      <td>0.619502</td>\n",
       "      <td>0.832919</td>\n",
       "      <td>0.710530</td>\n",
       "      <td>0.637063</td>\n",
       "      <td>0.858008</td>\n",
       "      <td>0.731209</td>\n",
       "      <td>0.725213</td>\n",
       "      <td>0.794655</td>\n",
       "      <td>0.758347</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.542200</td>\n",
       "      <td>0.508795</td>\n",
       "      <td>0.843675</td>\n",
       "      <td>0.630215</td>\n",
       "      <td>0.721487</td>\n",
       "      <td>0.805741</td>\n",
       "      <td>0.522908</td>\n",
       "      <td>0.634220</td>\n",
       "      <td>0.818953</td>\n",
       "      <td>0.624590</td>\n",
       "      <td>0.708686</td>\n",
       "      <td>0.617276</td>\n",
       "      <td>0.829926</td>\n",
       "      <td>0.707977</td>\n",
       "      <td>0.606598</td>\n",
       "      <td>0.898345</td>\n",
       "      <td>0.724192</td>\n",
       "      <td>0.723738</td>\n",
       "      <td>0.796982</td>\n",
       "      <td>0.758596</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.004596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.483249</td>\n",
       "      <td>0.830984</td>\n",
       "      <td>0.660978</td>\n",
       "      <td>0.736295</td>\n",
       "      <td>0.792370</td>\n",
       "      <td>0.574937</td>\n",
       "      <td>0.666365</td>\n",
       "      <td>0.867117</td>\n",
       "      <td>0.602756</td>\n",
       "      <td>0.711163</td>\n",
       "      <td>0.626970</td>\n",
       "      <td>0.842961</td>\n",
       "      <td>0.719096</td>\n",
       "      <td>0.659946</td>\n",
       "      <td>0.859367</td>\n",
       "      <td>0.746568</td>\n",
       "      <td>0.668312</td>\n",
       "      <td>0.854228</td>\n",
       "      <td>0.749918</td>\n",
       "      <td>0.345052</td>\n",
       "      <td>0.043751</td>\n",
       "      <td>0.077655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.470391</td>\n",
       "      <td>0.837287</td>\n",
       "      <td>0.674012</td>\n",
       "      <td>0.746829</td>\n",
       "      <td>0.795970</td>\n",
       "      <td>0.575260</td>\n",
       "      <td>0.667852</td>\n",
       "      <td>0.871863</td>\n",
       "      <td>0.599314</td>\n",
       "      <td>0.710342</td>\n",
       "      <td>0.630994</td>\n",
       "      <td>0.848371</td>\n",
       "      <td>0.723712</td>\n",
       "      <td>0.662963</td>\n",
       "      <td>0.875690</td>\n",
       "      <td>0.754621</td>\n",
       "      <td>0.725101</td>\n",
       "      <td>0.824010</td>\n",
       "      <td>0.771397</td>\n",
       "      <td>0.321726</td>\n",
       "      <td>0.102196</td>\n",
       "      <td>0.155118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>0.456466</td>\n",
       "      <td>0.833024</td>\n",
       "      <td>0.687790</td>\n",
       "      <td>0.753472</td>\n",
       "      <td>0.790141</td>\n",
       "      <td>0.592294</td>\n",
       "      <td>0.677060</td>\n",
       "      <td>0.892880</td>\n",
       "      <td>0.594925</td>\n",
       "      <td>0.714067</td>\n",
       "      <td>0.635690</td>\n",
       "      <td>0.854684</td>\n",
       "      <td>0.729097</td>\n",
       "      <td>0.677390</td>\n",
       "      <td>0.868999</td>\n",
       "      <td>0.761323</td>\n",
       "      <td>0.731299</td>\n",
       "      <td>0.826293</td>\n",
       "      <td>0.775899</td>\n",
       "      <td>0.400452</td>\n",
       "      <td>0.102278</td>\n",
       "      <td>0.162940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.411800</td>\n",
       "      <td>0.448102</td>\n",
       "      <td>0.849731</td>\n",
       "      <td>0.675679</td>\n",
       "      <td>0.752775</td>\n",
       "      <td>0.835492</td>\n",
       "      <td>0.570810</td>\n",
       "      <td>0.678242</td>\n",
       "      <td>0.870797</td>\n",
       "      <td>0.612929</td>\n",
       "      <td>0.719454</td>\n",
       "      <td>0.637731</td>\n",
       "      <td>0.857428</td>\n",
       "      <td>0.731438</td>\n",
       "      <td>0.674746</td>\n",
       "      <td>0.879770</td>\n",
       "      <td>0.763737</td>\n",
       "      <td>0.715574</td>\n",
       "      <td>0.843902</td>\n",
       "      <td>0.774458</td>\n",
       "      <td>0.289926</td>\n",
       "      <td>0.241126</td>\n",
       "      <td>0.263283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.370600</td>\n",
       "      <td>0.450311</td>\n",
       "      <td>0.869983</td>\n",
       "      <td>0.660575</td>\n",
       "      <td>0.750953</td>\n",
       "      <td>0.862141</td>\n",
       "      <td>0.559867</td>\n",
       "      <td>0.678876</td>\n",
       "      <td>0.838046</td>\n",
       "      <td>0.627054</td>\n",
       "      <td>0.717357</td>\n",
       "      <td>0.636501</td>\n",
       "      <td>0.855774</td>\n",
       "      <td>0.730027</td>\n",
       "      <td>0.651027</td>\n",
       "      <td>0.910957</td>\n",
       "      <td>0.759364</td>\n",
       "      <td>0.723961</td>\n",
       "      <td>0.838655</td>\n",
       "      <td>0.777098</td>\n",
       "      <td>0.302020</td>\n",
       "      <td>0.212316</td>\n",
       "      <td>0.249345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.362300</td>\n",
       "      <td>0.433413</td>\n",
       "      <td>0.876545</td>\n",
       "      <td>0.659984</td>\n",
       "      <td>0.753003</td>\n",
       "      <td>0.826328</td>\n",
       "      <td>0.581157</td>\n",
       "      <td>0.682389</td>\n",
       "      <td>0.850929</td>\n",
       "      <td>0.631410</td>\n",
       "      <td>0.724915</td>\n",
       "      <td>0.640653</td>\n",
       "      <td>0.861357</td>\n",
       "      <td>0.734789</td>\n",
       "      <td>0.660553</td>\n",
       "      <td>0.906592</td>\n",
       "      <td>0.764259</td>\n",
       "      <td>0.676549</td>\n",
       "      <td>0.882308</td>\n",
       "      <td>0.765849</td>\n",
       "      <td>0.335308</td>\n",
       "      <td>0.182186</td>\n",
       "      <td>0.236093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.342200</td>\n",
       "      <td>0.438401</td>\n",
       "      <td>0.884187</td>\n",
       "      <td>0.658818</td>\n",
       "      <td>0.755044</td>\n",
       "      <td>0.858214</td>\n",
       "      <td>0.560818</td>\n",
       "      <td>0.678352</td>\n",
       "      <td>0.839436</td>\n",
       "      <td>0.641063</td>\n",
       "      <td>0.726959</td>\n",
       "      <td>0.641864</td>\n",
       "      <td>0.862984</td>\n",
       "      <td>0.736178</td>\n",
       "      <td>0.662655</td>\n",
       "      <td>0.911627</td>\n",
       "      <td>0.767453</td>\n",
       "      <td>0.672621</td>\n",
       "      <td>0.888495</td>\n",
       "      <td>0.765631</td>\n",
       "      <td>0.334658</td>\n",
       "      <td>0.257140</td>\n",
       "      <td>0.290822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.322400</td>\n",
       "      <td>0.423365</td>\n",
       "      <td>0.872737</td>\n",
       "      <td>0.687302</td>\n",
       "      <td>0.768998</td>\n",
       "      <td>0.848302</td>\n",
       "      <td>0.566633</td>\n",
       "      <td>0.679431</td>\n",
       "      <td>0.873981</td>\n",
       "      <td>0.624364</td>\n",
       "      <td>0.728379</td>\n",
       "      <td>0.647941</td>\n",
       "      <td>0.871155</td>\n",
       "      <td>0.743148</td>\n",
       "      <td>0.697098</td>\n",
       "      <td>0.896876</td>\n",
       "      <td>0.784467</td>\n",
       "      <td>0.704444</td>\n",
       "      <td>0.874578</td>\n",
       "      <td>0.780345</td>\n",
       "      <td>0.301037</td>\n",
       "      <td>0.335480</td>\n",
       "      <td>0.317326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.317400</td>\n",
       "      <td>0.410590</td>\n",
       "      <td>0.866834</td>\n",
       "      <td>0.690135</td>\n",
       "      <td>0.768457</td>\n",
       "      <td>0.869559</td>\n",
       "      <td>0.581697</td>\n",
       "      <td>0.697079</td>\n",
       "      <td>0.880892</td>\n",
       "      <td>0.622026</td>\n",
       "      <td>0.729165</td>\n",
       "      <td>0.649332</td>\n",
       "      <td>0.873026</td>\n",
       "      <td>0.744744</td>\n",
       "      <td>0.703257</td>\n",
       "      <td>0.886350</td>\n",
       "      <td>0.784258</td>\n",
       "      <td>0.695974</td>\n",
       "      <td>0.881581</td>\n",
       "      <td>0.777858</td>\n",
       "      <td>0.308759</td>\n",
       "      <td>0.334654</td>\n",
       "      <td>0.321185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.408316</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>0.676254</td>\n",
       "      <td>0.766859</td>\n",
       "      <td>0.851559</td>\n",
       "      <td>0.606789</td>\n",
       "      <td>0.708633</td>\n",
       "      <td>0.865504</td>\n",
       "      <td>0.630343</td>\n",
       "      <td>0.729438</td>\n",
       "      <td>0.650130</td>\n",
       "      <td>0.874098</td>\n",
       "      <td>0.745659</td>\n",
       "      <td>0.681188</td>\n",
       "      <td>0.913142</td>\n",
       "      <td>0.780291</td>\n",
       "      <td>0.693648</td>\n",
       "      <td>0.885463</td>\n",
       "      <td>0.777905</td>\n",
       "      <td>0.326028</td>\n",
       "      <td>0.286115</td>\n",
       "      <td>0.304770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.338500</td>\n",
       "      <td>0.407588</td>\n",
       "      <td>0.888907</td>\n",
       "      <td>0.674914</td>\n",
       "      <td>0.767268</td>\n",
       "      <td>0.874562</td>\n",
       "      <td>0.559836</td>\n",
       "      <td>0.682671</td>\n",
       "      <td>0.855402</td>\n",
       "      <td>0.642253</td>\n",
       "      <td>0.733659</td>\n",
       "      <td>0.649664</td>\n",
       "      <td>0.873473</td>\n",
       "      <td>0.745125</td>\n",
       "      <td>0.684257</td>\n",
       "      <td>0.914318</td>\n",
       "      <td>0.782732</td>\n",
       "      <td>0.695679</td>\n",
       "      <td>0.885541</td>\n",
       "      <td>0.779211</td>\n",
       "      <td>0.295332</td>\n",
       "      <td>0.362473</td>\n",
       "      <td>0.325476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.398570</td>\n",
       "      <td>0.881906</td>\n",
       "      <td>0.680384</td>\n",
       "      <td>0.768147</td>\n",
       "      <td>0.843035</td>\n",
       "      <td>0.618200</td>\n",
       "      <td>0.713320</td>\n",
       "      <td>0.877273</td>\n",
       "      <td>0.628214</td>\n",
       "      <td>0.732142</td>\n",
       "      <td>0.651982</td>\n",
       "      <td>0.876588</td>\n",
       "      <td>0.747783</td>\n",
       "      <td>0.689218</td>\n",
       "      <td>0.905968</td>\n",
       "      <td>0.782867</td>\n",
       "      <td>0.688870</td>\n",
       "      <td>0.892612</td>\n",
       "      <td>0.777616</td>\n",
       "      <td>0.311288</td>\n",
       "      <td>0.280915</td>\n",
       "      <td>0.295322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.270500</td>\n",
       "      <td>0.398161</td>\n",
       "      <td>0.882480</td>\n",
       "      <td>0.688597</td>\n",
       "      <td>0.773575</td>\n",
       "      <td>0.857949</td>\n",
       "      <td>0.590161</td>\n",
       "      <td>0.699295</td>\n",
       "      <td>0.882302</td>\n",
       "      <td>0.632464</td>\n",
       "      <td>0.736779</td>\n",
       "      <td>0.654733</td>\n",
       "      <td>0.880287</td>\n",
       "      <td>0.750938</td>\n",
       "      <td>0.702174</td>\n",
       "      <td>0.903354</td>\n",
       "      <td>0.790159</td>\n",
       "      <td>0.694942</td>\n",
       "      <td>0.885195</td>\n",
       "      <td>0.778614</td>\n",
       "      <td>0.332551</td>\n",
       "      <td>0.409609</td>\n",
       "      <td>0.367079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.237000</td>\n",
       "      <td>0.401686</td>\n",
       "      <td>0.869346</td>\n",
       "      <td>0.700783</td>\n",
       "      <td>0.776016</td>\n",
       "      <td>0.843471</td>\n",
       "      <td>0.614762</td>\n",
       "      <td>0.711180</td>\n",
       "      <td>0.898789</td>\n",
       "      <td>0.615982</td>\n",
       "      <td>0.730985</td>\n",
       "      <td>0.654213</td>\n",
       "      <td>0.879588</td>\n",
       "      <td>0.750341</td>\n",
       "      <td>0.718606</td>\n",
       "      <td>0.883956</td>\n",
       "      <td>0.792750</td>\n",
       "      <td>0.694095</td>\n",
       "      <td>0.893115</td>\n",
       "      <td>0.781127</td>\n",
       "      <td>0.334800</td>\n",
       "      <td>0.377002</td>\n",
       "      <td>0.354649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.257600</td>\n",
       "      <td>0.418259</td>\n",
       "      <td>0.889184</td>\n",
       "      <td>0.682206</td>\n",
       "      <td>0.772063</td>\n",
       "      <td>0.894525</td>\n",
       "      <td>0.562118</td>\n",
       "      <td>0.690393</td>\n",
       "      <td>0.859240</td>\n",
       "      <td>0.640391</td>\n",
       "      <td>0.733846</td>\n",
       "      <td>0.652257</td>\n",
       "      <td>0.876958</td>\n",
       "      <td>0.748098</td>\n",
       "      <td>0.694311</td>\n",
       "      <td>0.912050</td>\n",
       "      <td>0.788423</td>\n",
       "      <td>0.705570</td>\n",
       "      <td>0.885732</td>\n",
       "      <td>0.785452</td>\n",
       "      <td>0.291403</td>\n",
       "      <td>0.418607</td>\n",
       "      <td>0.343610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.236300</td>\n",
       "      <td>0.410307</td>\n",
       "      <td>0.885220</td>\n",
       "      <td>0.686021</td>\n",
       "      <td>0.772993</td>\n",
       "      <td>0.858214</td>\n",
       "      <td>0.611633</td>\n",
       "      <td>0.714240</td>\n",
       "      <td>0.880226</td>\n",
       "      <td>0.630920</td>\n",
       "      <td>0.735007</td>\n",
       "      <td>0.655094</td>\n",
       "      <td>0.880773</td>\n",
       "      <td>0.751352</td>\n",
       "      <td>0.688015</td>\n",
       "      <td>0.914931</td>\n",
       "      <td>0.785411</td>\n",
       "      <td>0.712379</td>\n",
       "      <td>0.875495</td>\n",
       "      <td>0.785559</td>\n",
       "      <td>0.351929</td>\n",
       "      <td>0.312448</td>\n",
       "      <td>0.331015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.228900</td>\n",
       "      <td>0.431767</td>\n",
       "      <td>0.897398</td>\n",
       "      <td>0.673566</td>\n",
       "      <td>0.769536</td>\n",
       "      <td>0.830941</td>\n",
       "      <td>0.647767</td>\n",
       "      <td>0.728008</td>\n",
       "      <td>0.864472</td>\n",
       "      <td>0.630464</td>\n",
       "      <td>0.729153</td>\n",
       "      <td>0.652817</td>\n",
       "      <td>0.877711</td>\n",
       "      <td>0.748741</td>\n",
       "      <td>0.680447</td>\n",
       "      <td>0.923128</td>\n",
       "      <td>0.783424</td>\n",
       "      <td>0.699648</td>\n",
       "      <td>0.891549</td>\n",
       "      <td>0.784026</td>\n",
       "      <td>0.310248</td>\n",
       "      <td>0.382368</td>\n",
       "      <td>0.342552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.410261</td>\n",
       "      <td>0.888591</td>\n",
       "      <td>0.680135</td>\n",
       "      <td>0.770512</td>\n",
       "      <td>0.844032</td>\n",
       "      <td>0.656943</td>\n",
       "      <td>0.738827</td>\n",
       "      <td>0.880623</td>\n",
       "      <td>0.628740</td>\n",
       "      <td>0.733664</td>\n",
       "      <td>0.655537</td>\n",
       "      <td>0.881369</td>\n",
       "      <td>0.751861</td>\n",
       "      <td>0.694884</td>\n",
       "      <td>0.907536</td>\n",
       "      <td>0.787099</td>\n",
       "      <td>0.698836</td>\n",
       "      <td>0.891281</td>\n",
       "      <td>0.783412</td>\n",
       "      <td>0.296049</td>\n",
       "      <td>0.457735</td>\n",
       "      <td>0.359551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.196800</td>\n",
       "      <td>0.435692</td>\n",
       "      <td>0.899652</td>\n",
       "      <td>0.671337</td>\n",
       "      <td>0.768903</td>\n",
       "      <td>0.869123</td>\n",
       "      <td>0.631071</td>\n",
       "      <td>0.731209</td>\n",
       "      <td>0.860025</td>\n",
       "      <td>0.640408</td>\n",
       "      <td>0.734144</td>\n",
       "      <td>0.654688</td>\n",
       "      <td>0.880227</td>\n",
       "      <td>0.750887</td>\n",
       "      <td>0.681792</td>\n",
       "      <td>0.922839</td>\n",
       "      <td>0.784210</td>\n",
       "      <td>0.697104</td>\n",
       "      <td>0.894984</td>\n",
       "      <td>0.783746</td>\n",
       "      <td>0.294645</td>\n",
       "      <td>0.431071</td>\n",
       "      <td>0.350035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>0.449576</td>\n",
       "      <td>0.902649</td>\n",
       "      <td>0.668541</td>\n",
       "      <td>0.768154</td>\n",
       "      <td>0.863310</td>\n",
       "      <td>0.646455</td>\n",
       "      <td>0.739308</td>\n",
       "      <td>0.856234</td>\n",
       "      <td>0.639089</td>\n",
       "      <td>0.731894</td>\n",
       "      <td>0.654213</td>\n",
       "      <td>0.879588</td>\n",
       "      <td>0.750341</td>\n",
       "      <td>0.673995</td>\n",
       "      <td>0.929153</td>\n",
       "      <td>0.781268</td>\n",
       "      <td>0.698017</td>\n",
       "      <td>0.891650</td>\n",
       "      <td>0.783040</td>\n",
       "      <td>0.321644</td>\n",
       "      <td>0.408866</td>\n",
       "      <td>0.360047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.200100</td>\n",
       "      <td>0.432218</td>\n",
       "      <td>0.887965</td>\n",
       "      <td>0.685069</td>\n",
       "      <td>0.773431</td>\n",
       "      <td>0.884567</td>\n",
       "      <td>0.623291</td>\n",
       "      <td>0.731292</td>\n",
       "      <td>0.874397</td>\n",
       "      <td>0.631255</td>\n",
       "      <td>0.733193</td>\n",
       "      <td>0.655949</td>\n",
       "      <td>0.881923</td>\n",
       "      <td>0.752334</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>0.903243</td>\n",
       "      <td>0.790963</td>\n",
       "      <td>0.694295</td>\n",
       "      <td>0.899548</td>\n",
       "      <td>0.783705</td>\n",
       "      <td>0.300759</td>\n",
       "      <td>0.471025</td>\n",
       "      <td>0.367110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.164200</td>\n",
       "      <td>0.441396</td>\n",
       "      <td>0.894592</td>\n",
       "      <td>0.681754</td>\n",
       "      <td>0.773804</td>\n",
       "      <td>0.860894</td>\n",
       "      <td>0.621509</td>\n",
       "      <td>0.721873</td>\n",
       "      <td>0.872105</td>\n",
       "      <td>0.634916</td>\n",
       "      <td>0.734844</td>\n",
       "      <td>0.656123</td>\n",
       "      <td>0.882156</td>\n",
       "      <td>0.752532</td>\n",
       "      <td>0.698130</td>\n",
       "      <td>0.914797</td>\n",
       "      <td>0.791911</td>\n",
       "      <td>0.699724</td>\n",
       "      <td>0.895442</td>\n",
       "      <td>0.785576</td>\n",
       "      <td>0.282699</td>\n",
       "      <td>0.449975</td>\n",
       "      <td>0.347241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.216500</td>\n",
       "      <td>0.420642</td>\n",
       "      <td>0.886115</td>\n",
       "      <td>0.690639</td>\n",
       "      <td>0.776259</td>\n",
       "      <td>0.866692</td>\n",
       "      <td>0.643632</td>\n",
       "      <td>0.738689</td>\n",
       "      <td>0.886382</td>\n",
       "      <td>0.626350</td>\n",
       "      <td>0.734016</td>\n",
       "      <td>0.657904</td>\n",
       "      <td>0.884551</td>\n",
       "      <td>0.754576</td>\n",
       "      <td>0.705509</td>\n",
       "      <td>0.904952</td>\n",
       "      <td>0.792881</td>\n",
       "      <td>0.683362</td>\n",
       "      <td>0.903173</td>\n",
       "      <td>0.778040</td>\n",
       "      <td>0.349672</td>\n",
       "      <td>0.351577</td>\n",
       "      <td>0.350621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.153600</td>\n",
       "      <td>0.444749</td>\n",
       "      <td>0.895863</td>\n",
       "      <td>0.681732</td>\n",
       "      <td>0.774265</td>\n",
       "      <td>0.867284</td>\n",
       "      <td>0.643379</td>\n",
       "      <td>0.738738</td>\n",
       "      <td>0.871809</td>\n",
       "      <td>0.632233</td>\n",
       "      <td>0.732940</td>\n",
       "      <td>0.656903</td>\n",
       "      <td>0.883205</td>\n",
       "      <td>0.753427</td>\n",
       "      <td>0.686488</td>\n",
       "      <td>0.922477</td>\n",
       "      <td>0.787176</td>\n",
       "      <td>0.694942</td>\n",
       "      <td>0.901226</td>\n",
       "      <td>0.784754</td>\n",
       "      <td>0.349540</td>\n",
       "      <td>0.278933</td>\n",
       "      <td>0.310270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.159400</td>\n",
       "      <td>0.462763</td>\n",
       "      <td>0.888555</td>\n",
       "      <td>0.690885</td>\n",
       "      <td>0.777350</td>\n",
       "      <td>0.864853</td>\n",
       "      <td>0.636622</td>\n",
       "      <td>0.733391</td>\n",
       "      <td>0.882477</td>\n",
       "      <td>0.626240</td>\n",
       "      <td>0.732598</td>\n",
       "      <td>0.657445</td>\n",
       "      <td>0.883934</td>\n",
       "      <td>0.754049</td>\n",
       "      <td>0.699261</td>\n",
       "      <td>0.910938</td>\n",
       "      <td>0.791186</td>\n",
       "      <td>0.700776</td>\n",
       "      <td>0.899682</td>\n",
       "      <td>0.787869</td>\n",
       "      <td>0.340368</td>\n",
       "      <td>0.320868</td>\n",
       "      <td>0.330330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>0.470439</td>\n",
       "      <td>0.882540</td>\n",
       "      <td>0.696362</td>\n",
       "      <td>0.778474</td>\n",
       "      <td>0.876105</td>\n",
       "      <td>0.616712</td>\n",
       "      <td>0.723872</td>\n",
       "      <td>0.888417</td>\n",
       "      <td>0.626439</td>\n",
       "      <td>0.734774</td>\n",
       "      <td>0.657847</td>\n",
       "      <td>0.884475</td>\n",
       "      <td>0.754510</td>\n",
       "      <td>0.708885</td>\n",
       "      <td>0.900606</td>\n",
       "      <td>0.793326</td>\n",
       "      <td>0.706021</td>\n",
       "      <td>0.896874</td>\n",
       "      <td>0.790085</td>\n",
       "      <td>0.326608</td>\n",
       "      <td>0.384844</td>\n",
       "      <td>0.353342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.168600</td>\n",
       "      <td>0.474423</td>\n",
       "      <td>0.898142</td>\n",
       "      <td>0.682365</td>\n",
       "      <td>0.775524</td>\n",
       "      <td>0.898733</td>\n",
       "      <td>0.586968</td>\n",
       "      <td>0.710139</td>\n",
       "      <td>0.858466</td>\n",
       "      <td>0.641039</td>\n",
       "      <td>0.733988</td>\n",
       "      <td>0.655547</td>\n",
       "      <td>0.881382</td>\n",
       "      <td>0.751872</td>\n",
       "      <td>0.702905</td>\n",
       "      <td>0.914580</td>\n",
       "      <td>0.794892</td>\n",
       "      <td>0.682389</td>\n",
       "      <td>0.908017</td>\n",
       "      <td>0.779198</td>\n",
       "      <td>0.304953</td>\n",
       "      <td>0.468631</td>\n",
       "      <td>0.369476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.142300</td>\n",
       "      <td>0.464948</td>\n",
       "      <td>0.886784</td>\n",
       "      <td>0.693981</td>\n",
       "      <td>0.778624</td>\n",
       "      <td>0.870634</td>\n",
       "      <td>0.615963</td>\n",
       "      <td>0.721484</td>\n",
       "      <td>0.880296</td>\n",
       "      <td>0.625407</td>\n",
       "      <td>0.731276</td>\n",
       "      <td>0.656484</td>\n",
       "      <td>0.882641</td>\n",
       "      <td>0.752946</td>\n",
       "      <td>0.709331</td>\n",
       "      <td>0.904160</td>\n",
       "      <td>0.794982</td>\n",
       "      <td>0.698484</td>\n",
       "      <td>0.900286</td>\n",
       "      <td>0.786648</td>\n",
       "      <td>0.324411</td>\n",
       "      <td>0.410187</td>\n",
       "      <td>0.362290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.473120</td>\n",
       "      <td>0.896118</td>\n",
       "      <td>0.675110</td>\n",
       "      <td>0.770070</td>\n",
       "      <td>0.885486</td>\n",
       "      <td>0.620803</td>\n",
       "      <td>0.729889</td>\n",
       "      <td>0.858790</td>\n",
       "      <td>0.637258</td>\n",
       "      <td>0.731621</td>\n",
       "      <td>0.654062</td>\n",
       "      <td>0.879385</td>\n",
       "      <td>0.750168</td>\n",
       "      <td>0.699302</td>\n",
       "      <td>0.911133</td>\n",
       "      <td>0.791285</td>\n",
       "      <td>0.697600</td>\n",
       "      <td>0.899514</td>\n",
       "      <td>0.785793</td>\n",
       "      <td>0.255117</td>\n",
       "      <td>0.545319</td>\n",
       "      <td>0.347611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.159900</td>\n",
       "      <td>0.473268</td>\n",
       "      <td>0.894603</td>\n",
       "      <td>0.686199</td>\n",
       "      <td>0.776663</td>\n",
       "      <td>0.890395</td>\n",
       "      <td>0.602559</td>\n",
       "      <td>0.718729</td>\n",
       "      <td>0.869298</td>\n",
       "      <td>0.637065</td>\n",
       "      <td>0.735280</td>\n",
       "      <td>0.657136</td>\n",
       "      <td>0.883518</td>\n",
       "      <td>0.753694</td>\n",
       "      <td>0.699099</td>\n",
       "      <td>0.912674</td>\n",
       "      <td>0.791735</td>\n",
       "      <td>0.697067</td>\n",
       "      <td>0.903911</td>\n",
       "      <td>0.787126</td>\n",
       "      <td>0.329633</td>\n",
       "      <td>0.433878</td>\n",
       "      <td>0.374639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>0.486894</td>\n",
       "      <td>0.900939</td>\n",
       "      <td>0.682549</td>\n",
       "      <td>0.776684</td>\n",
       "      <td>0.876993</td>\n",
       "      <td>0.620694</td>\n",
       "      <td>0.726913</td>\n",
       "      <td>0.863222</td>\n",
       "      <td>0.635220</td>\n",
       "      <td>0.731874</td>\n",
       "      <td>0.656670</td>\n",
       "      <td>0.882891</td>\n",
       "      <td>0.753160</td>\n",
       "      <td>0.688865</td>\n",
       "      <td>0.926161</td>\n",
       "      <td>0.790079</td>\n",
       "      <td>0.682156</td>\n",
       "      <td>0.909684</td>\n",
       "      <td>0.779659</td>\n",
       "      <td>0.420396</td>\n",
       "      <td>0.289252</td>\n",
       "      <td>0.342706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c0e527a271d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    288\u001b[0m )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1270\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1732\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1734\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1767\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/environment/python/versions/miniconda3-4.7.12/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-c0e527a271d4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 active_labels = torch.where(\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mactive_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 )\n\u001b[1;32m     67\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertPreTrainedModel\n",
    "from transformers import DistilBertModel,DistilBertConfig,Trainer,TrainingArguments,set_seed,AutoModelForTokenClassification\n",
    "from transformers import DistilBertForTokenClassification\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# set_seed(12)\n",
    "\n",
    "class DistilBertForTokenClassification(DistilBertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
    "            1]``.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.distilbert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "model=DistilBertForTokenClassification.from_pretrained(\"distilbert-base-cased\",num_labels=8)\n",
    "\n",
    "# config=DistilBertConfig.from_pretrained(\"distilbert-base-cased\",num_labels=8+1)\n",
    "# model=TestModel(config)\n",
    "# print(config)\n",
    "# state_dict=torch.load(\"C:\\\\Users\\\\tom\\\\.cache\\\\huggingface\\\\transformers\\\\9c9f39769dba4c5fe379b4bc82973eb01297bd607954621434eb9f1bc85a23a0.06b428c87335c1bb22eae46fdab31c8286efa0aa09e898a7ac42ddf5c3f5dc19\",map_location=\"cpu\")\n",
    "# model.load_state_dict(state_dict,strict=False)\n",
    "# print(state_dict)\n",
    "# model.tie_weights()\n",
    "# model.eval()\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "\n",
    "winds=np.load(\"./winds.npy\")\n",
    "attns=np.load(\"./attns.npy\")\n",
    "sequs=np.load(\"./sequs.npy\")\n",
    "winds2=np.load(\"./winds2.npy\")\n",
    "attns2=np.load(\"./attns2.npy\")\n",
    "sequs2=np.load(\"./sequs2.npy\")\n",
    "\n",
    "class POLDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids,attention_mask,labels):\n",
    "        self.input_ids=input_ids\n",
    "        self.attention_mask=attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"input_ids\":self.input_ids[idx],\"attention_mask\":self.attention_mask[idx]}\n",
    "        item['labels'] = torch.LongTensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def compute_metrics3(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    content_all = 0\n",
    "    content_right = 0\n",
    "\n",
    "    source_all = 0\n",
    "    source_right = 0\n",
    "\n",
    "    none_all = 0\n",
    "    none_right = 0\n",
    "\n",
    "    content_all2 = 0\n",
    "    content_right2 = 0\n",
    "\n",
    "    source_all2 = 0\n",
    "    source_right2 = 0\n",
    "\n",
    "    none_all2 = 0\n",
    "    none_right2 = 0\n",
    "\n",
    "    left_all = 0\n",
    "    left_right = 0\n",
    "\n",
    "    right_all = 0\n",
    "    right_right = 0\n",
    "\n",
    "    vain_all = 0\n",
    "    vain_right = 0\n",
    "\n",
    "    left_all2 = 0\n",
    "    left_right2 = 0\n",
    "\n",
    "    right_all2 = 0\n",
    "    right_right2 = 0\n",
    "\n",
    "    vain_all2 = 0\n",
    "    vain_right2 = 0\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        for j in range(labels.shape[1]):\n",
    "            if labels[i][j] == 1 or labels[i][j] == 2 or labels[i][j] == 4 or labels[i][j] == 5 or labels[i][j] == 6 or \\\n",
    "                    labels[i][j] == 7:\n",
    "                content_all += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    content_right += 1\n",
    "            elif labels[i][j] == 3:\n",
    "                source_all += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    source_right += 1\n",
    "            elif labels[i][j] == 0:\n",
    "                none_all += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    none_right += 1\n",
    "\n",
    "            if labels[i][j] == 4 or labels[i][j] == 5:\n",
    "                left_all += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    left_right += 1\n",
    "\n",
    "            if labels[i][j] == 6 or labels[i][j] == 7:\n",
    "                right_all += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    right_right += 1\n",
    "\n",
    "            if labels[i][j] == 1 or labels[i][j] == 2:\n",
    "                vain_all += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    vain_right += 1\n",
    "\n",
    "            if predictions[i][j] == 1 or predictions[i][j] == 2 or predictions[i][j] == 4 or predictions[i][j] == 5 or \\\n",
    "                    predictions[i][j] == 6 or predictions[i][j] == 7:\n",
    "                content_all2 += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    content_right2 += 1\n",
    "            elif predictions[i][j] == 3:\n",
    "                source_all2 += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    source_right2 += 1\n",
    "            elif predictions[i][j] == 0:\n",
    "                none_all2 += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    none_right2 += 1\n",
    "\n",
    "            if predictions[i][j] == 4 or predictions[i][j] == 5:\n",
    "                left_all2 += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    left_right2 += 1\n",
    "\n",
    "            if predictions[i][j] == 6 or predictions[i][j] == 7:\n",
    "                right_all2 += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    right_right2 += 1\n",
    "\n",
    "            if predictions[i][j] == 1 or predictions[i][j] == 2:\n",
    "                vain_all2 += 1\n",
    "                if labels[i][j] == predictions[i][j]:\n",
    "                    vain_right2 += 1\n",
    "\n",
    "    content_recall = content_right / content_all\n",
    "    content_precision = content_right2 / content_all2\n",
    "    content_f1 = 2 * content_recall * content_precision / (content_recall + content_precision + 0.000001)\n",
    "    source_recall = source_right / source_all\n",
    "    source_precision = source_right2 / source_all2\n",
    "    source_f1 = 2 * source_recall * source_precision / (source_recall + source_precision + 0.000001)\n",
    "    none_recall = none_right / none_all\n",
    "    none_precision = none_right2 / none_all2\n",
    "    none_f1 = 2 * none_recall * none_precision / (none_recall + none_precision + 0.000001)\n",
    "    overall_precision = (content_right2 + source_right2 + none_right2) / (content_all2 + source_all2 + none_all2)\n",
    "    overall_recall = (content_right + source_right + none_right) / (content_all + source_all + none_all)\n",
    "    overall_f1 = 2 * overall_recall * overall_precision / (overall_recall + overall_precision + 0.000001)\n",
    "\n",
    "    left_recall = left_right / left_all\n",
    "    left_precision = left_right2 / left_all2\n",
    "    left_f1 = 2 * left_recall * left_precision / (left_recall + left_precision + 0.000001)\n",
    "\n",
    "    right_recall = right_right / right_all\n",
    "    right_precision = right_right2 / right_all2\n",
    "    right_f1 = 2 * right_recall * right_precision / (right_recall + right_precision + 0.000001)\n",
    "\n",
    "    vain_recall = vain_right / (vain_all + 1)\n",
    "    vain_precision = vain_right2 / (vain_all2 + 1)\n",
    "    vain_f1 = 2 * vain_recall * vain_precision / (vain_precision + vain_recall + 0.000001)\n",
    "\n",
    "    return {\n",
    "        \"ContentRecall\": content_recall,\n",
    "        \"ContentPrecision\": content_precision,\n",
    "        \"ContentF1\": content_f1,\n",
    "        \"SourceRecall\": source_recall,\n",
    "        \"SourcePrecision\": source_precision,\n",
    "        \"SourceF1\": source_f1,\n",
    "        \"NoneRecall\": none_recall,\n",
    "        \"NonePrecision\": none_precision,\n",
    "        \"NoneF1\": none_f1,\n",
    "        \"OverallPrecision\": overall_precision,\n",
    "        \"OverallRecall\": overall_recall,\n",
    "        \"OverallF1\": overall_f1,\n",
    "        \"LeftPrecision\": left_precision,\n",
    "        \"LeftRecall\": left_recall,\n",
    "        \"LeftF1\": left_f1,\n",
    "        \"RightPrecision\": right_precision,\n",
    "        \"RightRecall\": right_recall,\n",
    "        \"RightF1\": right_f1,\n",
    "        \"VainPrecision\": vain_precision,\n",
    "        \"VainRecall\": vain_recall,\n",
    "        \"VainF1\": vain_f1\n",
    "    }\n",
    "\n",
    "train_dataset = POLDataset(winds,attns,sequs)\n",
    "dev_dataset = POLDataset(winds2,attns2,sequs2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=8,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=2000,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    save_steps=100,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=dev_dataset,\n",
    "    compute_metrics=compute_metrics3\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "warming-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertPreTrainedModel\n",
    "from transformers import DistilBertModel,DistilBertConfig,Trainer,TrainingArguments,set_seed,AutoModelForTokenClassification\n",
    "from transformers import DistilBertForTokenClassification\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "demonstrated-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DistilBertForTokenClassification.from_pretrained(\"checkpoint-3300/\",num_labels=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cross-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accessible-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "central-yeast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 07:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]/S! Ariel/S! Mal/S! ##ka/S! ,/S! a/S! professor/S! at/S! Yes/S! ##hiva/S! University/S! and/S! an/S! author/S! of/S! “/S! Who/S! is/C← open/C← to/C← author/C← ##itarian/C← governance/C← within/C← western/C← demo/C← ##c/C← ##rac/C← ##ies/C← ?/C← ”/S! agreed in an email that/C!← both/C← liberal/C← ##s/C← and/C← conservative/C← ##s/C← “/C← engage/C← in/C← bias/C← ##ed/C← reasoning/C← on/C← the/C← basis/C← of/C← partisan/C← ##ship/C← ,/C← ”/C← but , he/S! argued , there/C!← is/C← still/C← a/C← fundamental/C← difference/C← between/C← left/C← and/C← right/C← :/C← [SEP]/C← "
     ]
    }
   ],
   "source": [
    "def displayResult(seqs,attns,predictions,no):\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    conv_dict={0:'',1:'/C!',2:\"/C\",3:\"/S!\",4:\"/C!←\",5:\"/C←\",6:\"/C!→\",7:\"/C!→\"}\n",
    "    for i in range(len(predictions[no])):\n",
    "        if attns[no][i]==0:\n",
    "            break\n",
    "        print(tokenizer.decode([seqs[no][i]])+conv_dict[predictions[no][i]],end=' ')       \n",
    "\n",
    "res=tokenizer(['Ariel Malka, a professor at Yeshiva University and an author of “Who is open to authoritarian governance within western democracies?” agreed in an email that both liberals and conservatives “engage in biased reasoning on the basis of partisanship,” but, he argued, there is still a fundamental difference between left and right:','I hate you.'],padding='max_length',max_length=511)\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids,attention_mask):\n",
    "        self.input_ids=input_ids\n",
    "        self.attention_mask=attention_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"input_ids\":self.input_ids[idx],\"attention_mask\":self.attention_mask[idx]}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "res_data=TestDataset(res['input_ids'],res[\"attention_mask\"])\n",
    "pred_res=trainer.predict(res_data)\n",
    "displayResult(res['input_ids'],res[\"attention_mask\"],pred_res[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "front-novel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayFormatResult(input_id,attention,prediction,offset_map,overall_offset):\n",
    "    result=[]\n",
    "    predict = np.argmax(prediction, axis=2)\n",
    "    for i in range(len(input_id)):        \n",
    "        tuple_list=[]\n",
    "        tuple_type=[]\n",
    "        j=0\n",
    "        while j<len(predict[i]):\n",
    "            if attention[i][j]==0:\n",
    "                break\n",
    "            if predict[i][j]==3:\n",
    "                left=j\n",
    "                while j<len(predict[i]) and predict[i][j] == 3:\n",
    "                    if attention[i][j]==0:\n",
    "                        break\n",
    "                    j+=1\n",
    "                tuple_list.append((left,j))\n",
    "                tuple_type.append(0) #说话人\n",
    "                \n",
    "            elif j<len(predict[i]) and predict[i][j] == 0:\n",
    "                j+=1\n",
    "                \n",
    "            elif predict[i][j] == 1 or predict[i][j] == 2:\n",
    "                left=j\n",
    "                while j<len(predict[i]) and (predict[i][j] == 1 or predict[i][j] == 2):\n",
    "                    if attention[i][j]==0:\n",
    "                        break\n",
    "                    j+=1\n",
    "                tuple_list.append((left,j))\n",
    "                tuple_type.append(1) #匿名\n",
    "                \n",
    "            elif predict[i][j] == 4 or predict[i][j] == 5:\n",
    "                left=j\n",
    "                while j<len(predict[i]) and (predict[i][j] == 4 or predict[i][j] == 5):\n",
    "                    if attention[i][j]==0:\n",
    "                        break\n",
    "                    j+=1\n",
    "                tuple_list.append((left,j))\n",
    "                tuple_type.append(2) #向左\n",
    "                \n",
    "            elif predict[i][j] == 6 or predict[i][j] == 7:\n",
    "                left=j\n",
    "                while j<len(predict[i]) and (predict[i][j] == 6 or predict[i][j] == 7):\n",
    "                    if attention[i][j]==0:\n",
    "                        break\n",
    "                    j+=1\n",
    "                tuple_list.append((left,j))\n",
    "                tuple_type.append(3) #向右\n",
    "                   \n",
    "        for t in range(len(tuple_list)):\n",
    "            if tuple_type[t]==0:\n",
    "                pass\n",
    "            \n",
    "            elif tuple_type[t]==1:\n",
    "                result.append({\"mention\":\"Unknown\",\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":-1,\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":-1,\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                               \"SegmentOffset\":overall_offset[i][0],\n",
    "                               \"Type\":\"Anonymous\"})\n",
    "                \n",
    "            elif tuple_type[t]==2:\n",
    "                back=t\n",
    "                while back>=0 and tuple_type[back]!=0:\n",
    "                    back-=1\n",
    "                if back<0:\n",
    "                    result.append({\"mention\":\"Unknown\",\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":-1,\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":-1,\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                                   \"SegmentOffset\":overall_offset[i][0],\n",
    "                               \"Type\":\"TowardsLeftFailed\"})\n",
    "                else:\n",
    "                    result.append({\"mention\":tokenizer.decode(input_id[i][tuple_list[back][0]:tuple_list[back][1]]),\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":offset_map[i][tuple_list[back][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":offset_map[i][tuple_list[back][1]-1][1]+overall_offset[i][0],\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                                   \"SegmentOffset\":overall_offset[i][0],\n",
    "                               \"Type\":\"TowardsLeftSucceeded\"   })\n",
    "                    \n",
    "            elif tuple_type[t]==3:\n",
    "                after=t\n",
    "                while after<len(tuple_type) and tuple_type[after]!=0:\n",
    "                    after+=1\n",
    "                if after>=len(tuple_type):\n",
    "                    result.append({\"mention\":\"Unknown\",\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":-1,\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":-1,\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                                   \"SegmentOffset\":overall_offset[i][0],\n",
    "                                  \"Type\":\"TowardsRightFailed\"})\n",
    "                else:\n",
    "                    result.append({\"mention\":tokenizer.decode(input_id[i][tuple_list[after][0]:tuple_list[after][1]]),\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":offset_map[i][tuple_list[after][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":offset_map[i][tuple_list[after][1]-1][1]+overall_offset[i][0],\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                                   \"SegmentOffset\":overall_offset[i][0],\n",
    "                                  \"Type\":\"TowardsRightSucceeded\"})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3b4407bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[[ 8.0605799e-01, -5.9098190e-01, -6.3269782e-01, ...,\n",
      "          6.3757771e-01,  4.0913954e-02, -2.0112686e-01],\n",
      "        [ 1.3090616e+00, -1.4902513e+00, -2.6270444e+00, ...,\n",
      "         -5.4884857e-01,  2.4629512e-01, -1.4280637e-01],\n",
      "        [ 9.6190464e-01, -2.0724247e+00, -1.6157204e+00, ...,\n",
      "         -7.3388630e-01, -1.2125616e+00,  5.8179826e-01],\n",
      "        ...,\n",
      "        [ 2.0274944e+00, -2.0934317e+00, -1.8715183e+00, ...,\n",
      "         -4.4806129e-01, -1.2577022e+00,  3.7372077e-01],\n",
      "        [ 1.8551995e+00, -2.0018098e+00, -1.8377572e+00, ...,\n",
      "         -3.3739111e-01, -1.1473535e+00,  3.0937505e-01],\n",
      "        [ 2.3822696e+00, -1.9338609e+00, -1.9312274e+00, ...,\n",
      "         -6.4932488e-02, -1.0994846e+00, -3.5829255e-01]],\n",
      "\n",
      "       [[ 1.9318455e-01, -4.3617716e-01, -7.0437558e-02, ...,\n",
      "          3.0385801e-01,  2.2475210e-01, -2.2860557e-01],\n",
      "        [ 3.2389009e-01, -9.1635817e-01, -3.1577757e-01, ...,\n",
      "          5.5821669e-01,  4.7580820e-02,  2.1291332e-01],\n",
      "        [ 1.5872552e+00, -1.5157242e+00,  1.4742784e-01, ...,\n",
      "          1.1657257e+00, -1.4400815e+00,  4.0357018e-01],\n",
      "        ...,\n",
      "        [ 6.0866690e-01, -6.6590810e-01, -8.7241292e-02, ...,\n",
      "          4.8179203e-01,  1.5973058e-02,  4.5230325e-02],\n",
      "        [ 5.9042954e-01, -6.5155387e-01, -9.5331080e-02, ...,\n",
      "          4.5271340e-01, -1.3592051e-03,  5.8215760e-02],\n",
      "        [ 5.9763020e-01, -6.4672881e-01, -8.4382907e-02, ...,\n",
      "          4.5026919e-01, -1.1517408e-02,  8.6385071e-02]]], dtype=float32), label_ids=None, metrics={'test_runtime': 3.5072, 'test_samples_per_second': 0.57, 'init_mem_cpu_alloc_delta': 938819584, 'init_mem_gpu_alloc_delta': 261313024, 'init_mem_cpu_peaked_delta': 265400320, 'init_mem_gpu_peaked_delta': 0, 'test_mem_cpu_alloc_delta': 142041088, 'test_mem_gpu_alloc_delta': 0, 'test_mem_cpu_peaked_delta': 1847296, 'test_mem_gpu_peaked_delta': 72326144})\n"
     ]
    }
   ],
   "source": [
    "print(pred_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a99bfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17214, 18880, 1968, 117]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['input_ids'][0][1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "209be000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← ./C← Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← this/C← remained/C← “/C← under/C← review/C← ”/C← . Boris/S! Johnson/S! told MPs this week that/C!← while/C← the/C← plan/C← was/C← still/C← for/C← local/C← and/C← mayor/C← ##al/C← elections/C← ,/C← some/C← of/C← which/C← were/C← postponed/C← from/C← last/C← year/C← ,/C← to/C← happen/C← on/C← 6/C← May/C← in/C← England/C← ,/C← [SEP]/C← "
     ]
    }
   ],
   "source": [
    "res=tokenizer(['Boris Johnson told MPs this week that while the plan was still for local and mayoral elections, some of which were postponed from last year, to happen on 6 May in England, this remained “under review”.'*100,'I hate you.'],padding='max_length',max_length=511,truncation=True)\n",
    "res_data=TestDataset(res['input_ids'],res[\"attention_mask\"])\n",
    "pred_res=trainer.predict(res_data)\n",
    "displayResult(res['input_ids'],res[\"attention_mask\"],pred_res[0],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00da455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(txt):\n",
    "    pos=0\n",
    "    segs=[]\n",
    "    offsets=[]\n",
    "    while pos<len(txt):\n",
    "        if txt[pos]=='\\n':\n",
    "            while pos<len(txt) and txt[pos]=='\\n':\n",
    "                pos+=1\n",
    "        else:\n",
    "            left=pos\n",
    "            while pos<len(txt) and txt[pos]!='\\n':\n",
    "                pos+=1\n",
    "            segs.append(txt[left:pos])\n",
    "            offsets.append((left,pos))\n",
    "    return segs,offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b404cc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Britain’s economy grew at a record quarterly rate of more than 15% as lockdown restrictions were eased in the summer but the recovery was losing momentum even before new curbs came in, the latest official figures have revealed.',\n",
       "  'Data from the Office for National Statistics showed that national output expanded by just 1.1% in September – the last month before fresh action was taken to limit the spread of Covid-19.',\n",
       "  'The ONS said that while the economy had now expanded for five months in a row, the pace of recovery had decelerated. Record growth in the July to September period followed an unprecedented drop of 19.8% in the second quarter and a fall of 2.5% in the first three months of the year.',\n",
       "  'Gross domestic product – the measure used to gauge the size of the economy – increased by 9.1% in June, 6.3% in July, and 2.2% in August before slowing again in September.',\n",
       "  'Gross domestic product (GDP) measures the total value of activity in the economy over a given period of time.\\xa0',\n",
       "  'Put simply, if GDP is up on the previous three months, the economy is growing; if it is down, it is contracting. Two or more consecutive quarters of contraction are considered to be a recession.\\xa0',\n",
       "  'GDP is the sum of all goods and services produced in the economy, including the service sector, manufacturing, construction, energy, agriculture and government. Several key activities are not counted, such as unpaid work in the home.\\xa0',\n",
       "  'The ONS uses three measures that should, in theory, add up to the same number.',\n",
       "  '• The value of all goods and services produced – known as the output or production measure.•\\xa0The value of the income generated from company profits and wages – known as the income measure.•\\xa0The value of goods and services purchased by households, government, business (in terms of investment in machinery and buildings) and from overseas – known as the expenditure measure.',\n",
       "  'Economists are concerned with the real rate of change of GDP, which accounts for how the economy is performing after inflation.',\n",
       "  \"Britain's government statistics body, the\\xa0Office for National Statistics, produces GDP figures on a monthly basis about six weeks after the end of the month. It compares the change in GDP month on month, as well as over a three-month period.\\xa0\",\n",
       "  'The ONS warns that changes on the month can prove volatile, preferring to assess economic performance over a three-month period as the wider period can smooth over irregularities.\\xa0',\n",
       "  'The most closely watched GDP figures are for the four quarters of the year; for the three months to March, June, September and December.',\n",
       "  'The figures are usually revised in subsequent months as more data from businesses and the government becomes available.\\xa0\\xa0',\n",
       "  'The ONS also calculates the size of the UK economy relative to the number of people living here. GDP per capita shows whether we are actually getting richer or poorer, by stripping out the impact of population changes. Richard Partington',\n",
       "  'The ONS said there was a boost from children going back to school, which had helped support activity, but there was a slowdown in business for pubs and restaurants due to the end of the “eat out to help out” scheme.',\n",
       "  'Despite the pickup in activity as the economy began to open up in the late spring and summer, the level of national output in the third quarter was 9.7% below where it was in the last three months of 2019.',\n",
       "  'Britain’s record compares unfavourably with other leading developed countries, most of which saw smaller falls in output in the second quarter and which have recouped more of the lost ground. The US has the least bad record, with GDP 3.5% below where it was at the end of 2019.',\n",
       "  'Britain has seen a 22.9% rebound in activity since the economy’s low point in April, but in September was still 8.2% below its level when the crisis began in February, the ONS said.',\n",
       "  'The services sector – which includes hospitality and leisure – has been the hardest hit and remains 8.8% lower than it was before the spring lockdown was imposed. Manufacturing (-8.1%) and construction (-7.3%) are also well below their level in the early part of the year.',\n",
       "  'Economists said there would be a further blow to the economy from the tougher local restrictions introduced in October and the four-week lockdown for England that began in early November. GDP is expected to fall again in the final three months of 2020.',\n",
       "  'Dean Turner, economist at UBS global wealth management, said: “In our view, the latest round of Covid related restrictions will lead to a contraction in the final quarter, leaving the economy around 11% smaller than at the start of the year.”',\n",
       "  'Rishi Sunak, the chancellor of the exchequer, said: “Today’s figures show that our economy was recovering over the summer, but started to slow going into autumn. The steps we’ve had to take since to halt the spread of the virus mean growth has likely slowed further since then.',\n",
       "  '“But there are reasons to be cautiously optimistic on the health side – including promising news on tests and vaccines. My economic priority continues to be jobs – that’s why we extended furlough through to March and I welcome the news today that nearly 20,000 new roles for young people have been created through our Kickstart scheme.',\n",
       "  '“There are still hard times ahead, but we will continue to support people through this and ensure nobody is left without hope or opportunity.”',\n",
       "  'Fresh official forecasts for the economy and the public finances will be published on 25 November to coincide with the unveiling of Sunak’s one-year spending round plan. The chancellor will announce a package of measures to protect jobs and expand public services',\n",
       "  'Jonathan Athow, the deputy national statistician for economic statistics, said: “While all main sectors of the economy continued to recover, the rate of growth slowed again with the economy still remaining well below its pre-pandemic peak.',\n",
       "  '“The return of children to school boosted activity in the education sector. Housebuilding also continued to recover, while business strengthened for lawyers and accountants after a poor August.',\n",
       "  '“However, pubs and restaurants saw less business, after the eat out to help out scheme ended, and accommodation saw less business after a successful summer.”'],\n",
       " [(0, 227),\n",
       "  (228, 415),\n",
       "  (417, 699),\n",
       "  (700, 871),\n",
       "  (872, 982),\n",
       "  (983, 1178),\n",
       "  (1179, 1413),\n",
       "  (1414, 1492),\n",
       "  (1493, 1866),\n",
       "  (1867, 1994),\n",
       "  (1995, 2237),\n",
       "  (2238, 2418),\n",
       "  (2419, 2555),\n",
       "  (2556, 2677),\n",
       "  (2678, 2915),\n",
       "  (2916, 3131),\n",
       "  (3132, 3337),\n",
       "  (3338, 3615),\n",
       "  (3616, 3797),\n",
       "  (3798, 4070),\n",
       "  (4071, 4323),\n",
       "  (4324, 4566),\n",
       "  (4567, 4844),\n",
       "  (4845, 5180),\n",
       "  (5181, 5323),\n",
       "  (5324, 5587),\n",
       "  (5588, 5827),\n",
       "  (5828, 6021),\n",
       "  (6022, 6179)])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(\"Britain’s economy grew at a record quarterly rate of more than 15% as lockdown restrictions were eased in the summer but the recovery was losing momentum even before new curbs came in, the latest official figures have revealed.\\nData from the Office for National Statistics showed that national output expanded by just 1.1% in September – the last month before fresh action was taken to limit the spread of Covid-19.\\n\\nThe ONS said that while the economy had now expanded for five months in a row, the pace of recovery had decelerated. Record growth in the July to September period followed an unprecedented drop of 19.8% in the second quarter and a fall of 2.5% in the first three months of the year.\\nGross domestic product – the measure used to gauge the size of the economy – increased by 9.1% in June, 6.3% in July, and 2.2% in August before slowing again in September.\\nGross domestic product (GDP) measures the total value of activity in the economy over a given period of time. \\nPut simply, if GDP is up on the previous three months, the economy is growing; if it is down, it is contracting. Two or more consecutive quarters of contraction are considered to be a recession. \\nGDP is the sum of all goods and services produced in the economy, including the service sector, manufacturing, construction, energy, agriculture and government. Several key activities are not counted, such as unpaid work in the home. \\nThe ONS uses three measures that should, in theory, add up to the same number.\\n• The value of all goods and services produced – known as the output or production measure.• The value of the income generated from company profits and wages – known as the income measure.• The value of goods and services purchased by households, government, business (in terms of investment in machinery and buildings) and from overseas – known as the expenditure measure.\\nEconomists are concerned with the real rate of change of GDP, which accounts for how the economy is performing after inflation.\\nBritain's government statistics body, the Office for National Statistics, produces GDP figures on a monthly basis about six weeks after the end of the month. It compares the change in GDP month on month, as well as over a three-month period. \\nThe ONS warns that changes on the month can prove volatile, preferring to assess economic performance over a three-month period as the wider period can smooth over irregularities. \\nThe most closely watched GDP figures are for the four quarters of the year; for the three months to March, June, September and December.\\nThe figures are usually revised in subsequent months as more data from businesses and the government becomes available.  \\nThe ONS also calculates the size of the UK economy relative to the number of people living here. GDP per capita shows whether we are actually getting richer or poorer, by stripping out the impact of population changes. Richard Partington\\nThe ONS said there was a boost from children going back to school, which had helped support activity, but there was a slowdown in business for pubs and restaurants due to the end of the “eat out to help out” scheme.\\nDespite the pickup in activity as the economy began to open up in the late spring and summer, the level of national output in the third quarter was 9.7% below where it was in the last three months of 2019.\\nBritain’s record compares unfavourably with other leading developed countries, most of which saw smaller falls in output in the second quarter and which have recouped more of the lost ground. The US has the least bad record, with GDP 3.5% below where it was at the end of 2019.\\nBritain has seen a 22.9% rebound in activity since the economy’s low point in April, but in September was still 8.2% below its level when the crisis began in February, the ONS said.\\nThe services sector – which includes hospitality and leisure – has been the hardest hit and remains 8.8% lower than it was before the spring lockdown was imposed. Manufacturing (-8.1%) and construction (-7.3%) are also well below their level in the early part of the year.\\nEconomists said there would be a further blow to the economy from the tougher local restrictions introduced in October and the four-week lockdown for England that began in early November. GDP is expected to fall again in the final three months of 2020.\\nDean Turner, economist at UBS global wealth management, said: “In our view, the latest round of Covid related restrictions will lead to a contraction in the final quarter, leaving the economy around 11% smaller than at the start of the year.”\\nRishi Sunak, the chancellor of the exchequer, said: “Today’s figures show that our economy was recovering over the summer, but started to slow going into autumn. The steps we’ve had to take since to halt the spread of the virus mean growth has likely slowed further since then.\\n“But there are reasons to be cautiously optimistic on the health side – including promising news on tests and vaccines. My economic priority continues to be jobs – that’s why we extended furlough through to March and I welcome the news today that nearly 20,000 new roles for young people have been created through our Kickstart scheme.\\n“There are still hard times ahead, but we will continue to support people through this and ensure nobody is left without hope or opportunity.”\\nFresh official forecasts for the economy and the public finances will be published on 25 November to coincide with the unveiling of Sunak’s one-year spending round plan. The chancellor will announce a package of measures to protect jobs and expand public services\\nJonathan Athow, the deputy national statistician for economic statistics, said: “While all main sectors of the economy continued to recover, the rate of growth slowed again with the economy still remaining well below its pre-pandemic peak.\\n“The return of children to school boosted activity in the education sector. Housebuilding also continued to recover, while business strengthened for lawyers and accountants after a poor August.\\n“However, pubs and restaurants saw less business, after the eat out to help out scheme ended, and accommodation saw less business after a successful summer.”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3a600da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=tokenizer(,padding='max_length',max_length=511,truncation=True,return_offsets_mapping=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "92e8efad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 5),\n",
       " (6, 13),\n",
       " (14, 18),\n",
       " (19, 22),\n",
       " (23, 27),\n",
       " (28, 32),\n",
       " (33, 37),\n",
       " (38, 43),\n",
       " (44, 47),\n",
       " (48, 52),\n",
       " (53, 56),\n",
       " (57, 62),\n",
       " (63, 66),\n",
       " (67, 72),\n",
       " (73, 76),\n",
       " (77, 82),\n",
       " (82, 84),\n",
       " (85, 94),\n",
       " (94, 95),\n",
       " (96, 100),\n",
       " (101, 103),\n",
       " (104, 109),\n",
       " (110, 114),\n",
       " (115, 124),\n",
       " (125, 129),\n",
       " (130, 134),\n",
       " (135, 139),\n",
       " (139, 140),\n",
       " (141, 143),\n",
       " (144, 150),\n",
       " (151, 153),\n",
       " (154, 155),\n",
       " (156, 159),\n",
       " (160, 162),\n",
       " (163, 170),\n",
       " (170, 171),\n",
       " (172, 176),\n",
       " (177, 185),\n",
       " (186, 187),\n",
       " (187, 192),\n",
       " (193, 199),\n",
       " (199, 200),\n",
       " (200, 201),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['offset_mapping'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59df58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractText(txt):\n",
    "    segs,offsets=segment(txt)\n",
    "    res=tokenizer(segs,padding='max_length',max_length=511,truncation=True,return_offsets_mapping=True)\n",
    "    res_data=TestDataset(res['input_ids'],res[\"attention_mask\"])\n",
    "    pred_res=trainer.predict(res_data)\n",
    "    return displayFormatResult(res['input_ids'],res[\"attention_mask\"],pred_res[0],res['offset_mapping'],offsets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88e9ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"The UK government has dropped its opposition to sharing criminal suspects’ DNA data with EU law enforcement bodies, in a potential olive branch ahead of further talks on post-Brexit security.\\nThe U-turn was announced by the Home Office minister James Brokenshire in a statement to parliament on Monday, the day of an EU deadline for the government to reveal whether it intended to comply with European law.\\n\\nThe decision is separate from negotiations on a future relationship with the EU, but could improve the mood of tense talks as the UK seeks a security deal that includes permanent exchange of DNA, fingerprint and other data.\\nUnder the former home secretary Theresa May, the UK opted into an EU crime-fighting system in 2015, in which member states exchange biometric data. In June 2019 the government began sharing DNA data of convicted criminals, but refused to share criminal suspects’ DNA.\\nThat exception has now been reversed. “It is the government’s intention to begin exchanging suspects’ data held in England, Wales and Northern Ireland with connected EU member states through Prüm,” Brokenshire said, a reference to the 2005 agreement named after the small German town where EU countries first agreed to exchange fingerprints, DNA and car number plates of criminals and suspects.\\nBrokenshire said consultation would continue with the Scottish government, as policing is a devolved issue.\\nThe DNA exchange system enables British police to check the genetic code of EU criminals and suspects in 15 minutes, compared with 143 days through the Interpol process, the Home Office said in 2016. Interpol said its DNA database now has an automatic response time of 15 minutes. \\nCivil liberties campaigners and some MPs had been concerned that safeguards on criminal suspects’ data were insufficient. Brokenshire said the government was satisfied with EU processes as extra safeguards had been put in place since 2015, including an independent oversight board and extra checks when minors were involved.\\n“Ensuring continued adherence to the UK’s scientific standards means there is a one in a billion chance that a UK DNA sample would be falsely matched with an overseas criminal investigation,” he told parliament.\\nThe UK’s current participation in EU police data sharing ends on 31 December when the Brexit transition period expires. The EU has previously warned that opposition to sharing criminal suspects’ DNA would be an obstacle to a future deal.\\nThe government has said that since joining the DNA exchange system last July, about 12,000 “hits” related to UK investigations had come from EU member states, citing progress into an unsolved sexual assault case in Glasgow in 2012. A hit is an anonymised yes/no result of a DNA match. If there is a positive result, police forces can request personal information, such as name and date of birth. The UK has provided EU law enforcement officials with 41,000 hits.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7999a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'speaker': 'The UK government',\n",
       "  'speakerBegin': 0,\n",
       "  'speakerEnd': 17,\n",
       "  'Quotation': 'sharing criminal suspects ’ DNA data with EU law enforcement bodies',\n",
       "  'QuotationBegin': 48,\n",
       "  'QuotationEnd': 114,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'the Home Office minister James Brokenshire',\n",
       "  'speakerBegin': 220,\n",
       "  'speakerEnd': 262,\n",
       "  'Quotation': 'The U - turn',\n",
       "  'QuotationBegin': 192,\n",
       "  'QuotationEnd': 202,\n",
       "  'SegmentOffset': 192,\n",
       "  'Type': 'TowardsRightSucceeded'},\n",
       " {'speaker': 'the government',\n",
       "  'speakerBegin': 333,\n",
       "  'speakerEnd': 347,\n",
       "  'Quotation': 'comply with European law',\n",
       "  'QuotationBegin': 381,\n",
       "  'QuotationEnd': 405,\n",
       "  'SegmentOffset': 192,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'Unknown',\n",
       "  'speakerBegin': -1,\n",
       "  'speakerEnd': -1,\n",
       "  'Quotation': 'EU crime - fighting system',\n",
       "  'QuotationBegin': 698,\n",
       "  'QuotationEnd': 722,\n",
       "  'SegmentOffset': 632,\n",
       "  'Type': 'TowardsLeftFailed'},\n",
       " {'speaker': 'Unknown',\n",
       "  'speakerBegin': -1,\n",
       "  'speakerEnd': -1,\n",
       "  'Quotation': '2015',\n",
       "  'QuotationBegin': 726,\n",
       "  'QuotationEnd': 730,\n",
       "  'SegmentOffset': 632,\n",
       "  'Type': 'TowardsLeftFailed'},\n",
       " {'speaker': 'Unknown',\n",
       "  'speakerBegin': -1,\n",
       "  'speakerEnd': -1,\n",
       "  'Quotation': 'which',\n",
       "  'QuotationBegin': 735,\n",
       "  'QuotationEnd': 740,\n",
       "  'SegmentOffset': 632,\n",
       "  'Type': 'TowardsLeftFailed'},\n",
       " {'speaker': 'Unknown',\n",
       "  'speakerBegin': -1,\n",
       "  'speakerEnd': -1,\n",
       "  'Quotation': 'biometric data',\n",
       "  'QuotationBegin': 764,\n",
       "  'QuotationEnd': 778,\n",
       "  'SegmentOffset': 632,\n",
       "  'Type': 'TowardsLeftFailed'},\n",
       " {'speaker': 'government',\n",
       "  'speakerBegin': 797,\n",
       "  'speakerEnd': 807,\n",
       "  'Quotation': 'to share criminal suspects ’ DNA',\n",
       "  'QuotationBegin': 867,\n",
       "  'QuotationEnd': 898,\n",
       "  'SegmentOffset': 632,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'Brokenshire',\n",
       "  'speakerBegin': 1098,\n",
       "  'speakerEnd': 1109,\n",
       "  'Quotation': '“ It is the government ’ s intention to begin exchanging suspects ’ data held in England, Wales and Northern Ireland with connected EU member states through Prüm, ”',\n",
       "  'QuotationBegin': 938,\n",
       "  'QuotationEnd': 1097,\n",
       "  'SegmentOffset': 900,\n",
       "  'Type': 'TowardsRightSucceeded'},\n",
       " {'speaker': 'Brokenshire',\n",
       "  'speakerBegin': 1098,\n",
       "  'speakerEnd': 1109,\n",
       "  'Quotation': '2005',\n",
       "  'QuotationBegin': 1135,\n",
       "  'QuotationEnd': 1139,\n",
       "  'SegmentOffset': 900,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'Brokenshire',\n",
       "  'speakerBegin': 1098,\n",
       "  'speakerEnd': 1109,\n",
       "  'Quotation': 'EU countries',\n",
       "  'QuotationBegin': 1190,\n",
       "  'QuotationEnd': 1202,\n",
       "  'SegmentOffset': 900,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'Brokenshire',\n",
       "  'speakerBegin': 1098,\n",
       "  'speakerEnd': 1109,\n",
       "  'Quotation': 'exchange fingerprints, DNA and car number plates of criminals and suspects',\n",
       "  'QuotationBegin': 1219,\n",
       "  'QuotationEnd': 1293,\n",
       "  'SegmentOffset': 900,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': '[CLS] Brokenshire',\n",
       "  'speakerBegin': 1295,\n",
       "  'speakerEnd': 1306,\n",
       "  'Quotation': 'consultation would continue with the Scottish government',\n",
       "  'QuotationBegin': 1312,\n",
       "  'QuotationEnd': 1368,\n",
       "  'SegmentOffset': 1295,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': '[CLS] Brokenshire',\n",
       "  'speakerBegin': 1295,\n",
       "  'speakerEnd': 1306,\n",
       "  'Quotation': 'de',\n",
       "  'QuotationBegin': 1387,\n",
       "  'QuotationEnd': 1389,\n",
       "  'SegmentOffset': 1295,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': '[CLS] Brokenshire',\n",
       "  'speakerBegin': 1295,\n",
       "  'speakerEnd': 1306,\n",
       "  'Quotation': 'issue',\n",
       "  'QuotationBegin': 1396,\n",
       "  'QuotationEnd': 1401,\n",
       "  'SegmentOffset': 1295,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'the Home Office',\n",
       "  'speakerBegin': 1573,\n",
       "  'speakerEnd': 1588,\n",
       "  'Quotation': 'The DNA exchange system enables British police to check the genetic code of EU criminals and suspects in 15 minutes, compared with 143 days through the Interpol process',\n",
       "  'QuotationBegin': 1403,\n",
       "  'QuotationEnd': 1571,\n",
       "  'SegmentOffset': 1403,\n",
       "  'Type': 'TowardsRightSucceeded'},\n",
       " {'speaker': 'Interpol',\n",
       "  'speakerBegin': 1603,\n",
       "  'speakerEnd': 1611,\n",
       "  'Quotation': 'its DNA database now has an automatic response time of 15 minutes',\n",
       "  'QuotationBegin': 1617,\n",
       "  'QuotationEnd': 1682,\n",
       "  'SegmentOffset': 1403,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'Civil liberties campaigners and some MPs',\n",
       "  'speakerBegin': 1685,\n",
       "  'speakerEnd': 1725,\n",
       "  'Quotation': 'that safeguards on criminal suspects ’ data were insufficient',\n",
       "  'QuotationBegin': 1745,\n",
       "  'QuotationEnd': 1805,\n",
       "  'SegmentOffset': 1685,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'Brokenshire',\n",
       "  'speakerBegin': 1807,\n",
       "  'speakerEnd': 1818,\n",
       "  'Quotation': 'the government was satisfied with EU processes as extra safeguards had been put in place since 2015, including an independent oversight board and extra checks when minors were involved',\n",
       "  'QuotationBegin': 1824,\n",
       "  'QuotationEnd': 2008,\n",
       "  'SegmentOffset': 1685,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'Brokenshire',\n",
       "  'speakerBegin': 1807,\n",
       "  'speakerEnd': 1818,\n",
       "  'Quotation': '[SEP]',\n",
       "  'QuotationBegin': 1685,\n",
       "  'QuotationEnd': 1685,\n",
       "  'SegmentOffset': 1685,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'he',\n",
       "  'speakerBegin': 2202,\n",
       "  'speakerEnd': 2204,\n",
       "  'Quotation': '“ Ensuring continued adherence to the UK ’ s scientific standards means there is a one in a billion chance that a UK DNA sample would be falsely matched with an overseas criminal investigation, ”',\n",
       "  'QuotationBegin': 2010,\n",
       "  'QuotationEnd': 2201,\n",
       "  'SegmentOffset': 2010,\n",
       "  'Type': 'TowardsRightSucceeded'},\n",
       " {'speaker': 'Unknown',\n",
       "  'speakerBegin': -1,\n",
       "  'speakerEnd': -1,\n",
       "  'Quotation': '[SEP]',\n",
       "  'QuotationBegin': 2010,\n",
       "  'QuotationEnd': 2010,\n",
       "  'SegmentOffset': 2010,\n",
       "  'Type': 'TowardsRightFailed'},\n",
       " {'speaker': 'The EU',\n",
       "  'speakerBegin': 2342,\n",
       "  'speakerEnd': 2348,\n",
       "  'Quotation': 'that opposition to sharing criminal suspects ’ DNA would be an obstacle to a future deal',\n",
       "  'QuotationBegin': 2371,\n",
       "  'QuotationEnd': 2458,\n",
       "  'SegmentOffset': 2222,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'The government',\n",
       "  'speakerBegin': 2460,\n",
       "  'speakerEnd': 2474,\n",
       "  'Quotation': 'that since joining the DNA exchange system last July, about 12, 000 “ hits ” related to UK investigations had come from EU member states',\n",
       "  'QuotationBegin': 2484,\n",
       "  'QuotationEnd': 2617,\n",
       "  'SegmentOffset': 2460,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'The government',\n",
       "  'speakerBegin': 2460,\n",
       "  'speakerEnd': 2474,\n",
       "  'Quotation': 'progress into an unsolved sexual assault case in Glasgow in 2012',\n",
       "  'QuotationBegin': 2626,\n",
       "  'QuotationEnd': 2690,\n",
       "  'SegmentOffset': 2460,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'police forces',\n",
       "  'speakerBegin': 2776,\n",
       "  'speakerEnd': 2789,\n",
       "  'Quotation': 'personal information',\n",
       "  'QuotationBegin': 2802,\n",
       "  'QuotationEnd': 2822,\n",
       "  'SegmentOffset': 2460,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'police forces',\n",
       "  'speakerBegin': 2776,\n",
       "  'speakerEnd': 2789,\n",
       "  'Quotation': 'name',\n",
       "  'QuotationBegin': 2832,\n",
       "  'QuotationEnd': 2836,\n",
       "  'SegmentOffset': 2460,\n",
       "  'Type': 'TowardsLeftSucceeded'},\n",
       " {'speaker': 'police forces',\n",
       "  'speakerBegin': 2776,\n",
       "  'speakerEnd': 2789,\n",
       "  'Quotation': 'date',\n",
       "  'QuotationBegin': 2841,\n",
       "  'QuotationEnd': 2845,\n",
       "  'SegmentOffset': 2460,\n",
       "  'Type': 'TowardsLeftSucceeded'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractText(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee58ca62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[2202:2204]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b168fce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The UK government has dropped its opposition to sharing criminal suspects’ DNA data with EU law enforcement bodies, in a potential olive branch ahead of further talks on post-Brexit security.',\n",
       "  'The U-turn was announced by the Home Office minister James Brokenshire in a statement to parliament on Monday, the day of an EU deadline for the government to reveal whether it intended to comply with European law.',\n",
       "  'The decision is separate from negotiations on a future relationship with the EU, but could improve the mood of tense talks as the UK seeks a security deal that includes permanent exchange of DNA, fingerprint and other data.',\n",
       "  'Under the former home secretary Theresa May, the UK opted into an EU crime-fighting system in 2015, in which member states exchange biometric data. In June 2019 the government began sharing DNA data of convicted criminals, but refused to share criminal suspects’ DNA.',\n",
       "  'That exception has now been reversed. “It is the government’s intention to begin exchanging suspects’ data held in England, Wales and Northern Ireland with connected EU member states through Prüm,” Brokenshire said, a reference to the 2005 agreement named after the small German town where EU countries first agreed to exchange fingerprints, DNA and car number plates of criminals and suspects.',\n",
       "  'Brokenshire said consultation would continue with the Scottish government, as policing is a devolved issue.',\n",
       "  'The DNA exchange system enables British police to check the genetic code of EU criminals and suspects in 15 minutes, compared with 143 days through the Interpol process, the Home Office said in 2016. Interpol said its DNA database now has an automatic response time of 15 minutes. ',\n",
       "  'Civil liberties campaigners and some MPs had been concerned that safeguards on criminal suspects’ data were insufficient. Brokenshire said the government was satisfied with EU processes as extra safeguards had been put in place since 2015, including an independent oversight board and extra checks when minors were involved.',\n",
       "  '“Ensuring continued adherence to the UK’s scientific standards means there is a one in a billion chance that a UK DNA sample would be falsely matched with an overseas criminal investigation,” he told parliament.',\n",
       "  'The UK’s current participation in EU police data sharing ends on 31 December when the Brexit transition period expires. The EU has previously warned that opposition to sharing criminal suspects’ DNA would be an obstacle to a future deal.',\n",
       "  'The government has said that since joining the DNA exchange system last July, about 12,000 “hits” related to UK investigations had come from EU member states, citing progress into an unsolved sexual assault case in Glasgow in 2012. A hit is an anonymised yes/no result of a DNA match. If there is a positive result, police forces can request personal information, such as name and date of birth. The UK has provided EU law enforcement officials with 41,000 hits.'],\n",
       " [(0, 191),\n",
       "  (192, 406),\n",
       "  (408, 631),\n",
       "  (632, 899),\n",
       "  (900, 1294),\n",
       "  (1295, 1402),\n",
       "  (1403, 1684),\n",
       "  (1685, 2009),\n",
       "  (2010, 2221),\n",
       "  (2222, 2459),\n",
       "  (2460, 2922)])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dcd7bca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The U-turn was announced by the Home Office minister James Brokenshire in a statement to parliament on Monday, the day of an EU deadline for the government to reveal whether it intended to comply with European law.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[192:406]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee1ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genre.entity_linking import get_end_to_end_prefix_allowed_tokens_fn_hf as get_prefix_allowed_tokens_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfabb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genre.utils import get_entity_spans_hf as get_entity_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "798e2fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genre.hf_model import GENRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1260ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from genre.trie import Trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90c08343",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"models/kilt_titles_trie_dict.pkl\", \"rb\") as f:\n",
    "    trie = Trie.load_from_dict(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e03ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/models/hf_e2e_entity_linking_wiki_abs/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'models/hf_e2e_entity_linking_wiki_abs'. Make sure that:\n\n- 'models/hf_e2e_entity_linking_wiki_abs' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'models/hf_e2e_entity_linking_wiki_abs' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m             \u001b[1;31m# Load from URL or cache if already cached\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             resolved_config_file = cached_path(\n\u001b[0m\u001b[0;32m    485\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1270\u001b[0m         \u001b[1;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1271\u001b[1;33m         output_path = get_from_cache(\n\u001b[0m\u001b[0;32m   1272\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1441\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m             \u001b[0metag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X-Linked-Etag\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ETag\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 943\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/models/hf_e2e_entity_linking_wiki_abs/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-19f01068ea7f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGENRE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"models/hf_e2e_entity_linking_wiki_abs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\FirstModel\\genre\\hf_model.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, model_name_or_path)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGENREHubInterface\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBartTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name_or_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   1065\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m             \u001b[0mconfig_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m             config, model_kwargs = cls.config_class.from_pretrained(\n\u001b[0m\u001b[0;32m   1068\u001b[0m                 \u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m                 \u001b[1;33m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \"\"\"\n\u001b[1;32m--> 427\u001b[1;33m         \u001b[0mconfig_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_type\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             logger.warn(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m                 \u001b[1;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m             )\n\u001b[1;32m--> 504\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load config for 'models/hf_e2e_entity_linking_wiki_abs'. Make sure that:\n\n- 'models/hf_e2e_entity_linking_wiki_abs' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'models/hf_e2e_entity_linking_wiki_abs' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "model = GENRE.from_pretrained(\"models/hf_e2e_entity_linking_wiki_abs\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1fb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"In 1921, Einstein received an Nobel Prize.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb7cd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entity_spans(model,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ebf8521",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_prefix_allowed_tokens_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-01c3e0310ccb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprefix_allowed_tokens_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_prefix_allowed_tokens_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m model.sample(\n\u001b[0;32m      4\u001b[0m     \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprefix_allowed_tokens_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix_allowed_tokens_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_prefix_allowed_tokens_fn' is not defined"
     ]
    }
   ],
   "source": [
    "prefix_allowed_tokens_fn = get_prefix_allowed_tokens_fn(model, sentences)\n",
    "\n",
    "model.sample(\n",
    "    sentences,\n",
    "    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f28716b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "what=get_entity_spans(model,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44ba90eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  7,\n",
       "  'List_of_prizes_awarded_at_the_1921_International_Academy_of_Arts_and_Sciences_Prize_in_Geometry_and_in_Physics_(Euclidean-Aschenfeldt)'),\n",
       " (9, 8, 'Albert_Einstein'),\n",
       " [30,\n",
       "  11,\n",
       "  'Nobel Prize in Physiology or Medicine (Aristotle-Eggs) Prize in Ephraim Eben-Müller-Kulmbach-Innsbruck-Künstler-Ausstellungen-Eisenkirchen (Einstein-Auerstedt-Königstuhl) Aarne-Egenstahl, Fürstenberg-Sonderburg-Köln-Ausserlingen, Künchenfeldt-Sündenburg-Fürstenried-Aarne, Köln bei Ernstenberg, Königsberg, Nürnbergen, Südbergen and Neustadt am Rhein-Oberfrohnsrücken bei Nahrungen, Düsseldorf and Nuremberg (Egen) Agrariane) A-Sinfonogründe, Kirchhainen, Potsdam and Neuburg (Energetik) A.S. Türkstahlsrüppen, Einstein, Einstein and Naturmüller, Einstein (Einherrschaft) in Gewerksrüttemberg) In Nüssenkirchein, Einstein-Einheinsthalen and Märzen, Dorek and Dorem, Einstein. In 1921, Einstein received an Nobel Prize.   -']]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "what[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c640f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 7, 'List_of_prizes_awarded_at_the_1921_International_Academy_of_Arts_and_Sciences_Prize_in_Geometry_and_in_Physics_(Euclidean-Aschenfeldt)')\n",
      "(9, 8, 'Albert_Einstein')\n",
      "[30, 11, 'Nobel Prize in Physiology or Medicine (Aristotle-Eggs) Prize in Ephraim Eben-Müller-Kulmbach-Innsbruck-Künstler-Ausstellungen-Eisenkirchen (Einstein-Auerstedt-Königstuhl) Aarne-Egenstahl, Fürstenberg-Sonderburg-Köln-Ausserlingen, Künchenfeldt-Sündenburg-Fürstenried-Aarne, Köln bei Ernstenberg, Königsberg, Nürnbergen, Südbergen and Neustadt am Rhein-Oberfrohnsrücken bei Nahrungen, Düsseldorf and Nuremberg (Egen) Agrariane) A-Sinfonogründe, Kirchhainen, Potsdam and Neuburg (Energetik) A.S. Türkstahlsrüppen, Einstein, Einstein and Naturmüller, Einstein (Einherrschaft) in Gewerksrüttemberg) In Nüssenkirchein, Einstein-Einheinsthalen and Märzen, Dorek and Dorem, Einstein. In 1921, Einstein received an Nobel Prize.   -']\n"
     ]
    }
   ],
   "source": [
    "for i in what[0]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d1d46e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function genre.entity_linking._get_end_to_end_prefix_allowed_tokens_fn.<locals>.prefix_allowed_tokens_fn(batch_id, sent)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_allowed_tokens_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f852c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genre.utils import get_markdown\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2a75ae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[In 1921](https://en.wikipedia.org/wiki/List_of_prizes_awarded_at_the_1921_International_Academy_of_Arts_and_Sciences_Prize_in_Geometry_and_in_Physics_(Euclidean-Aschenfeldt)), [Einstein](https://en.wikipedia.org/wiki/Albert_Einstein) received an [Nobel Prize](https://en.wikipedia.org/wiki/Nobel Prize in Physiology or Medicine (Aristotle-Eggs) Prize in Ephraim Eben-Müller-Kulmbach-Innsbruck-Künstler-Ausstellungen-Eisenkirchen (Einstein-Auerstedt-Königstuhl) Aarne-Egenstahl, Fürstenberg-Sonderburg-Köln-Ausserlingen, Künchenfeldt-Sündenburg-Fürstenried-Aarne, Köln bei Ernstenberg, Königsberg, Nürnbergen, Südbergen and Neustadt am Rhein-Oberfrohnsrücken bei Nahrungen, Düsseldorf and Nuremberg (Egen) Agrariane) A-Sinfonogründe, Kirchhainen, Potsdam and Neuburg (Energetik) A.S. Türkstahlsrüppen, Einstein, Einstein and Naturmüller, Einstein (Einherrschaft) in Gewerksrüttemberg) In Nüssenkirchein, Einstein-Einheinsthalen and Märzen, Dorek and Dorem, Einstein. In 1921, Einstein received an Nobel Prize.   -)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(get_markdown(sentences, what)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e210c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genre.utils import get_entity_spans_pre_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "868ebbc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' In 1921, Einstein received an Nobel Prize. ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_entity_spans_pre_processing(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ad10b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'In { 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel } [ Nobel Prize in Physics ] Prize.',\n",
       "   'logprob': tensor(-0.9352)}],\n",
       " [{'text': 'In { 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel Prize } [ Nobel Memorial Prize in Economic Sciences (Euclidean Society) ] {. } [ Einstein (crater) ]',\n",
       "   'logprob': tensor(-1.1662)}],\n",
       " [{'text': 'In { 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel Prize } [ Nobel Memorial Prize in Economic Sciences (Euclidean League) ] {. } [ Einstein (crater) ]',\n",
       "   'logprob': tensor(-1.1767)}],\n",
       " [{'text': 'In { 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel Prize } [ Nobel Memorial Prize in Economic Sciences (Euclidean Division) ] {. } [ Einstein (crater) ]',\n",
       "   'logprob': tensor(-1.1864)}],\n",
       " [{'text': 'In { 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel Prize } [ Nobel Memorial Prize in Economic Sciences (Euclidean League) ] {. } [ Einstein Prize in Physicist ]',\n",
       "   'logprob': tensor(-1.3396)}]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_allowed_tokens_fn = get_prefix_allowed_tokens_fn(model, sentences)\n",
    "\n",
    "model.sample(\n",
    "    sentences,\n",
    "    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef0ad5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': ' { In 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel Prize } [ Nobel Prize in Physicist (1921 Prize in Physics) (1923) ] {. } [ Einstein (lunar crater) ] ',\n",
       "   'logprob': tensor(-1.6385)}],\n",
       " [{'text': ' { In 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel Prize } [ Nobel Prize in Physicist (1921 Prize in Physics) (1923) ] {. } [ Einstein (crater) ] ',\n",
       "   'logprob': tensor(-1.6445)}],\n",
       " [{'text': ' { In 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel Prize } [ Nobel Prize in Physicist (1921 Prize in Physics) (1923) ] {. } [ Einstein (lunar crater, Mount Everest) ] ',\n",
       "   'logprob': tensor(-1.7248)}],\n",
       " [{'text': ' { In 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel Prize } [ Nobel Prize in Physicist (1921 Prize in Physics) (1923) ] {. } [ Einstein (lunar crater-cutters) ] ',\n",
       "   'logprob': tensor(-1.7357)}],\n",
       " [{'text': ' { In 1921 } [ List of Nobel laureates in Physiology or Medicine by year of appointment ], { Einstein } [ Albert Einstein ] received an { Nobel Prize } [ Nobel Prize in Physicist (1921 Prize in Physics) (1923) ] {. } [ Einstein (lunar crater-cutter) ] ',\n",
       "   'logprob': tensor(-1.7435)}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_allowed_tokens_fn = get_prefix_allowed_tokens_fn(model, get_entity_spans_pre_processing(sentences))\n",
    "\n",
    "model.sample(\n",
    "    sentences,\n",
    "    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e07d7a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-00b058f30991>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_entity_spans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\FirstModel\\genre\\utils.py\u001b[0m in \u001b[0;36mget_entity_spans_hf\u001b[1;34m(model, input_sentences, mention_trie, candidates_trie, mention_to_candidates_dict, redirections)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[0mredirections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m ):\n\u001b[1;32m--> 178\u001b[1;33m     return _get_entity_spans(\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0minput_sentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\FirstModel\\genre\\utils.py\u001b[0m in \u001b[0;36m_get_entity_spans\u001b[1;34m(model, input_sentences, prefix_allowed_tokens_fn, redirections)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[0mredirections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m ):\n\u001b[1;32m--> 134\u001b[1;33m     output_sentences = model.sample(\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mget_entity_spans_pre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mprefix_allowed_tokens_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix_allowed_tokens_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Downloads\\FirstModel\\genre\\hf_model.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, sentences, num_beams, num_return_sequences, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m         }\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         outputs = self.generate(\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[1;33m**\u001b[0m\u001b[0minput_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mmin_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_encoder_decoder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             )\n\u001b[1;32m-> 1034\u001b[1;33m             return self.beam_search(\n\u001b[0m\u001b[0;32m   1035\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[1;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   1769\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1771\u001b[1;33m             outputs = self(\n\u001b[0m\u001b[0;32m   1772\u001b[0m                 \u001b[1;33m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1773\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1282\u001b[0m                 )\n\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1284\u001b[1;33m         outputs = self.model(\n\u001b[0m\u001b[0;32m   1285\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1286\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m         \u001b[1;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[0;32m   1176\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[0;32m   1043\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[0;32m    395\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 259\u001b[1;33m         \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_proj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_weights_reshaped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_entity_spans(model,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48deb604",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_prefix_allowed_tokens_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-883669f806f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"The EU\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprefix_allowed_tokens_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_prefix_allowed_tokens_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_entity_spans_pre_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m model.sample(\n\u001b[0;32m      5\u001b[0m     \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_prefix_allowed_tokens_fn' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = [\"The EU\"]\n",
    "prefix_allowed_tokens_fn = get_prefix_allowed_tokens_fn(model, get_entity_spans_pre_processing(sentences))\n",
    "\n",
    "model.sample(\n",
    "    sentences,\n",
    "    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d84a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from genre.trie import Trie\n",
    "\n",
    "# load the prefix tree (trie)\n",
    "with open(\"models/kilt_titles_trie_dict.pkl\", \"rb\") as f:\n",
    "    trie = Trie.load_from_dict(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2be7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genre.hf_model import GENRE\n",
    "model = GENRE.from_pretrained(\"models/hf_entity_disambiguation_aidayago\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0aeda09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'Donald Trump', 'logprob': tensor(-0.0784)}],\n",
       " [{'text': 'Presidency of Donald Trump', 'logprob': tensor(-1.0255)}],\n",
       " [{'text': 'President of the United States', 'logprob': tensor(-1.3235)}],\n",
       " [{'text': 'Presidency of Barack Obama', 'logprob': tensor(-2.0674)}],\n",
       " [{'text': 'Donald Trump in popular culture', 'logprob': tensor(-2.2210)}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"[START_ENT] Trump [END_ENT] is our president.\"]\n",
    "\n",
    "model.sample(\n",
    "    sentences,\n",
    "    prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3318e1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-cdf32aacf899>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "what[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6476bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "memo={}\n",
    "def getEntity(txt):\n",
    "    if txt in memo:\n",
    "        return memo[txt]\n",
    "    else:\n",
    "        if type(txt)!=str:\n",
    "            raise Exception(\"说话人不是字符串\")\n",
    "        else:\n",
    "            sentences = [\"[START_ENT] \"+txt+\" [END_ENT]\"]\n",
    "            result=model.sample(sentences,prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()))\n",
    "            return result[0][0]['text'],result[0][0]['logprob'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36e2ce2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Donald Trump', -0.12928657233715057)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getEntity(\"Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e30d7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://en.wikipedia.org/wiki/Donald_Trump'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"https://en.wikipedia.org/wiki/\"+'Donald Trump '.strip().replace(' ',\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d15ea392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertPreTrainedModel\n",
    "from transformers import DistilBertModel,DistilBertConfig,Trainer,TrainingArguments,set_seed,AutoModelForTokenClassification\n",
    "from transformers import DistilBertForTokenClassification\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "from genre.trie import Trie\n",
    "from genre.hf_model import GENRE\n",
    "\n",
    "with open(\"kilt_titles_trie_dict.pkl\", \"rb\") as f:\n",
    "    trie = Trie.load_from_dict(pickle.load(f))\n",
    "\n",
    "EDmodel = GENRE.from_pretrained(\"models/hf_entity_disambiguation_aidayago\").eval()\n",
    "\n",
    "model=DistilBertForTokenClassification.from_pretrained(\"checkpoint-3300/\",num_labels=8)\n",
    "trainer = Trainer(model=model)\n",
    "tokenizer=AutoTokenizer.from_pretrained('distilbert-base-cased')\n",
    "memo={}\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, input_ids,attention_mask):\n",
    "        self.input_ids=input_ids\n",
    "        self.attention_mask=attention_mask\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\"input_ids\":self.input_ids[idx],\"attention_mask\":self.attention_mask[idx]}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fceafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(txt):\n",
    "    pos=0\n",
    "    segs=[]\n",
    "    offsets=[]\n",
    "    while pos<len(txt):\n",
    "        if txt[pos]=='\\n':\n",
    "            while pos<len(txt) and txt[pos]=='\\n':\n",
    "                pos+=1\n",
    "        else:\n",
    "            left=pos\n",
    "            while pos<len(txt) and txt[pos]!='\\n':\n",
    "                pos+=1\n",
    "            segs.append(txt[left:pos])\n",
    "            offsets.append((left,pos))\n",
    "    return segs,offsets\n",
    "\n",
    "def displayFormatResult(input_id,attention,prediction,offset_map,overall_offset):\n",
    "    result=[]\n",
    "    predict = np.argmax(prediction, axis=2)\n",
    "    for i in range(len(input_id)):        \n",
    "        tuple_list=[]\n",
    "        tuple_type=[]\n",
    "        j=0\n",
    "        while j<len(predict[i]):\n",
    "            if attention[i][j]==0:\n",
    "                break\n",
    "            if predict[i][j]==3:\n",
    "                left=j\n",
    "                while j<len(predict[i]) and predict[i][j] == 3:\n",
    "                    if attention[i][j]==0:\n",
    "                        break\n",
    "                    j+=1\n",
    "                tuple_list.append((left,j))\n",
    "                tuple_type.append(0) #说话人\n",
    "                \n",
    "            elif j<len(predict[i]) and predict[i][j] == 0:\n",
    "                j+=1\n",
    "                \n",
    "            elif predict[i][j] == 1 or predict[i][j] == 2:\n",
    "                left=j\n",
    "                while j<len(predict[i]) and (predict[i][j] == 1 or predict[i][j] == 2):\n",
    "                    if attention[i][j]==0:\n",
    "                        break\n",
    "                    j+=1\n",
    "                tuple_list.append((left,j))\n",
    "                tuple_type.append(1) #匿名\n",
    "                \n",
    "            elif predict[i][j] == 4 or predict[i][j] == 5:\n",
    "                left=j\n",
    "                while j<len(predict[i]) and (predict[i][j] == 4 or predict[i][j] == 5):\n",
    "                    if attention[i][j]==0:\n",
    "                        break\n",
    "                    j+=1\n",
    "                tuple_list.append((left,j))\n",
    "                tuple_type.append(2) #向左\n",
    "                \n",
    "            elif predict[i][j] == 6 or predict[i][j] == 7:\n",
    "                left=j\n",
    "                while j<len(predict[i]) and (predict[i][j] == 6 or predict[i][j] == 7):\n",
    "                    if attention[i][j]==0:\n",
    "                        break\n",
    "                    j+=1\n",
    "                tuple_list.append((left,j))\n",
    "                tuple_type.append(3) #向右\n",
    "                   \n",
    "        for t in range(len(tuple_list)):\n",
    "            if tuple_type[t]==0:\n",
    "                pass\n",
    "            \n",
    "            elif tuple_type[t]==1:\n",
    "                result.append({\"mentionRaw\":\"Unknown\",\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":-1,\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":-1,\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                               \"SegmentOffset\":overall_offset[i][0],\n",
    "                               \"Type\":\"Anonymous\"})\n",
    "                \n",
    "            elif tuple_type[t]==2:\n",
    "                back=t\n",
    "                while back>=0 and tuple_type[back]!=0:\n",
    "                    back-=1\n",
    "                if back<0:\n",
    "                    result.append({\"mentionRaw\":\"Unknown\",\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":-1,\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":-1,\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                                   \"SegmentOffset\":overall_offset[i][0],\n",
    "                               \"Type\":\"TowardsLeftFailed\"})\n",
    "                else:\n",
    "                    result.append({\"mentionRaw\":tokenizer.decode(input_id[i][tuple_list[back][0]:tuple_list[back][1]]),\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":offset_map[i][tuple_list[back][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":offset_map[i][tuple_list[back][1]-1][1]+overall_offset[i][0],\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                                   \"SegmentOffset\":overall_offset[i][0],\n",
    "                               \"Type\":\"TowardsLeftSucceeded\"   })\n",
    "                    \n",
    "            elif tuple_type[t]==3:\n",
    "                after=t\n",
    "                while after<len(tuple_type) and tuple_type[after]!=0:\n",
    "                    after+=1\n",
    "                if after>=len(tuple_type):\n",
    "                    result.append({\"mentionRaw\":\"Unknown\",\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":-1,\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":-1,\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                                   \"SegmentOffset\":overall_offset[i][0],\n",
    "                                  \"Type\":\"TowardsRightFailed\"})\n",
    "                else:\n",
    "                    result.append({\"mentionRaw\":tokenizer.decode(input_id[i][tuple_list[after][0]:tuple_list[after][1]]),\n",
    "                               \"quoteSpeakerCharOffsetsFirst\":offset_map[i][tuple_list[after][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteSpeakerCharOffsetsSecond\":offset_map[i][tuple_list[after][1]-1][1]+overall_offset[i][0],\n",
    "                               \"quotation\":tokenizer.decode(input_id[i][tuple_list[t][0]:tuple_list[t][1]]),\n",
    "                               \"quoteCharOffsetsFirst\":offset_map[i][tuple_list[t][0]][0]+overall_offset[i][0],\n",
    "                               \"quoteCharOffsetsSecond\":offset_map[i][tuple_list[t][1]-1][1]+overall_offset[i][0],\n",
    "                                   \"SegmentOffset\":overall_offset[i][0],\n",
    "                                  \"Type\":\"TowardsRightSucceeded\"})\n",
    "    return result\n",
    "\n",
    "def getEntity(txt):\n",
    "    if txt in memo:\n",
    "        return memo[txt]\n",
    "    else:\n",
    "        if type(txt)!=str:\n",
    "            raise Exception(\"说话人不是字符串\")\n",
    "        else:\n",
    "            sentences = [\"[START_ENT] \"+txt+\" [END_ENT]\"]\n",
    "            result=EDmodel.sample(sentences,prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()))\n",
    "            memo[txt]=(result[0][0]['text'],result[0][0]['logprob'].item())\n",
    "            return result[0][0]['text'],result[0][0]['logprob'].item()\n",
    "\n",
    "def extractText(\n",
    "    txt):\n",
    "    segs,offsets=segment(txt)\n",
    "    res=tokenizer(segs,padding='max_length',max_length=511,truncation=True,return_offsets_mapping=True)\n",
    "    res_data=TestDataset(res['input_ids'],res[\"attention_mask\"])\n",
    "    pred_res=trainer.predict(res_data)\n",
    "    middle_result=displayFormatResult(res['input_ids'],res[\"attention_mask\"],pred_res[0],res['offset_mapping'],offsets)\n",
    "    for i in range(len(middle_result)):\n",
    "        if type(middle_result[i]['mentionRaw'])!=str:\n",
    "            print(\"说话人不是字符串！\")\n",
    "        else:\n",
    "            linked=getEntity(middle_result[i]['mentionRaw'])\n",
    "            middle_result[i]['mention']=linked[0]\n",
    "            middle_result[i]['mentionLinkLogProb']=linked[1]\n",
    "            middle_result[i]['links']=\"https://en.wikipedia.org/wiki/\"+linked[0].strip().replace(' ',\"_\")\n",
    "    return middle_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e27906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"President Donald Trump is expected to cut a significant number of U.S. troops in Afghanistan and a smaller number in Iraq by the final days of his presidency, U.S. officials said Monday. The plan would run counter to military commanders’ advice over the past year, while still falling short of Trump’s much-touted goal to end America’s long wars.The decision comes just days after Trump installed a new slate of loyalists in top Pentagon positions who share his frustration with the continued troop presence in the war zones. But the expected plans would leave 2,500 troops in both Iraq and Afghanistan, meaning that President-elect Joe Biden would be the fourth president to grapple with the still-smoldering conflicts launched in the aftermath of the Sept. 11, 2001, attacks.ADVERTISEMENTU.S. officials said military leaders were told over the weekend about the planned withdrawals and that an executive order is in the works but has not yet been delivered to commanders. Officials cautioned that there could always be changes, and Trump is known to make snap decisions based on media reports and online chatter. Officials spoke on condition of anonymity to discuss internal deliberations.There are 4,500 to 5,000 troops in Afghanistan and more than 3,000 in Iraq. As news broke about the plan, Republican leaders on Capitol Hill issued stark warnings about making any hasty exit from Afghanistan that could jeopardize the peace process and undermine counterterrorism efforts.More Stories:– NATO, acting US Pentagon chief discuss Afghanistan– Afghans welcome report on Australian troops' alleged crimes– Suicide car bomb in Afghan capital kills 3 troops, wounds 4Senate Majority Leader Mitch McConnell said the Trump administration has made tremendous headway against terrorist threats, but warned against a potentially “humiliating” pullout from Afghanistan that he said would be worse than President Barack Obama’s 2011 withdrawal from Iraq and reminiscent of the U.S. departure from Saigon in 1975.Rep. Michael McCaul, Republican leader on the House Foreign Affairs Committee, said of the plans for Afghanistan, “We need to ensure a residual force is maintained for the foreseeable future to protect U.S. national and homeland security interests and to help secure peace for Afghanistan.”Under the planned order, the troop cuts would be completed just five days before Biden takes office on Jan. 20. Military commanders have expressed less concern about the reduction in Iraq, where the Iraqi forces are better able to maintain their nation’s security.Trump’s new Pentagon chief, Christopher Miller, hinted at the troop withdrawals over the weekend in a carefully worded message to the force.ADVERTISEMENT“We remain committed to finishing the war that al-Qaida brought to our shores in 2001,” he said, and warned that “we must avoid our past strategic error of failing to see the fight through to the finish.”But Miller also made it clear that “all wars must end.”“This fight has been long, our sacrifices have been enormous. and many are weary of war — I’m one of them,” he said. ”Ending wars requires compromise and partnership. We met the challenge; we gave it our all. Now, it’s time to come home.”The accelerated withdrawal, however, goes against the longstanding advice of Trump’s military leadership, including Marine Gen. Frank McKenzie, top U.S. commander for the Middle East. But officials suggested that commanders will be able to live with the partial pullout, which allows them to keep counterterrorism troops in Afghanistan and gives them time to remove critical equipment fro the country.McKenzie and others have repeatedly argued that a hasty withdrawal could undercut negotiations to finalize ongoing peace negotiations between the Taliban and representatives of Afghan society, including the Afghan government. And they also warn that U.S. forces should remain in the country to keep Islamic State militants in check.Biden has sounded less absolute about troop withdrawal. He has said some troops could stay in Afghanistan to focus on the counterterrorism mission. In response to a questionnaire before the election, he said: “Americans are rightly weary of our longest war; I am, too. But we must end the war responsibly, in a manner that ensures we both guard against threats to our homeland and never have to go back.”The expected order, first reported by CNN, adds to what has been a litany of muddled White House and Pentagon messages on troops withdrawals from both Afghanistan and Iraq, only exacerbating what has been an emotional roller coaster for the troops and their families. Adding to the confusion: The Pentagon has historically failed to count up to hundreds of troops actually on the ground, including some special operations forces and personnel on temporary duty for only a few months. Often that is due to political sensitivities in those countries and in the U.S.The Pentagon was already on track to cut troops levels in Afghanistan to about 4,500 by mid-November. U.S. military leaders have consistently said that going below that number must be based on conditions on the ground, including a measurable reduction in attacks by the Taliban on Afghan troops. And they insist they have not seen that yet.America’s exit from Afghanistan after 19 years was laid out in a February agreement Washington reached with the Taliban. That agreement said U.S. troops would be out of Afghanistan in 18 months, provided the Taliban honored a commitment to fight terrorist groups, with most attention seemingly focused on the Islamic State group’s affiliate in the country.Military officials also have warned that there is a large amount of critical, classified equipment in Afghanistan that must be removed, but it will take time. They also say that any full U.S. withdrawal needs to be coordinated with other coalition allies that have troops in the country.The White House, however, issued a confusing series of statements about Afghanistan over the past month. Trump on Oct. 7 tweeted that “we should have the small remaining number of our BRAVE Men and Women serving in Afghanistan home by Christmas.” When asked about those comments, Robert O’Brien, his national security adviser, said Trump was just expressing a hope.O’Brien, meanwhile, has said the number of troops in Afghanistan would drop to 2,500 by early next year. At the time, defense officials said they had not received orders to cut troops to 2,500. And they warned that withdrawing troops quickly could remove some incentive for the struggling peace talks.According to the February agreement, the U.S. troop withdrawal next year is tied to the Taliban’s commitment to fight militant groups — such as the Islamic State group — in the county, and is not linked to successful negotiations between the Taliban and government. The Islamic State group is seen as extremely dangerous and intent on targeting America and other Western interests.The Taliban and Afghan government negotiators have been meeting for over a month in the Middle Eastern state of Qatar with little sign of progress. The Taliban, meanwhile, have staged near daily deadly attacks against Afghan forces. ____Associated Press writers Kathy Gannon in Islamabad, Pakistan, and Lisa Mascaro in Washington contributed to this report.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03d71be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mentionRaw': 'U. S. officials',\n",
       "  'quoteSpeakerCharOffsetsFirst': 159,\n",
       "  'quoteSpeakerCharOffsetsSecond': 173,\n",
       "  'quotation': 'President Donald Trump is expected to cut a significant number of U. S. troops in Afghanistan and a smaller number in Iraq by the final days of his presidency',\n",
       "  'quoteCharOffsetsFirst': 0,\n",
       "  'quoteCharOffsetsSecond': 157,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsRightSucceeded',\n",
       "  'mention': 'United States',\n",
       "  'mentionLinkLogProb': -0.44718435406684875,\n",
       "  'links': 'https://en.wikipedia.org/wiki/United_States'},\n",
       " {'mentionRaw': 'Trump ’ s',\n",
       "  'quoteSpeakerCharOffsetsFirst': 294,\n",
       "  'quoteSpeakerCharOffsetsSecond': 301,\n",
       "  'quotation': 'to end America ’ s long wars',\n",
       "  'quoteCharOffsetsFirst': 319,\n",
       "  'quoteCharOffsetsSecond': 345,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Donald Trump',\n",
       "  'mentionLinkLogProb': -0.568838894367218,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Donald_Trump'},\n",
       " {'mentionRaw': 'who',\n",
       "  'quoteSpeakerCharOffsetsFirst': 448,\n",
       "  'quoteSpeakerCharOffsetsSecond': 451,\n",
       "  'quotation': 'with the continued troop presence in the war zones',\n",
       "  'quoteCharOffsetsFirst': 474,\n",
       "  'quoteCharOffsetsSecond': 524,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Who (pronoun)',\n",
       "  'mentionLinkLogProb': -0.5766157507896423,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Who_(pronoun)'},\n",
       " {'mentionRaw': 'Unknown',\n",
       "  'quoteSpeakerCharOffsetsFirst': -1,\n",
       "  'quoteSpeakerCharOffsetsSecond': -1,\n",
       "  'quotation': 'leave 2, 500',\n",
       "  'quoteCharOffsetsFirst': 555,\n",
       "  'quoteCharOffsetsSecond': 566,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'Anonymous',\n",
       "  'mention': 'Unknown',\n",
       "  'mentionLinkLogProb': -0.9229084253311157,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Unknown'},\n",
       " {'mentionRaw': 'Unknown',\n",
       "  'quoteSpeakerCharOffsetsFirst': -1,\n",
       "  'quoteSpeakerCharOffsetsSecond': -1,\n",
       "  'quotation': 'in both Iraq and Afghanistan',\n",
       "  'quoteCharOffsetsFirst': 574,\n",
       "  'quoteCharOffsetsSecond': 602,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'Anonymous',\n",
       "  'mention': 'Unknown',\n",
       "  'mentionLinkLogProb': -0.9229084253311157,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Unknown'},\n",
       " {'mentionRaw': 'S. officials',\n",
       "  'quoteSpeakerCharOffsetsFirst': 792,\n",
       "  'quoteSpeakerCharOffsetsSecond': 804,\n",
       "  'quotation': 'military leaders were told over the weekend about the planned withdrawals and that an executive order is in the works but has not yet been delivered to commanders',\n",
       "  'quoteCharOffsetsFirst': 810,\n",
       "  'quoteCharOffsetsSecond': 972,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'South Korea',\n",
       "  'mentionLinkLogProb': -0.40526676177978516,\n",
       "  'links': 'https://en.wikipedia.org/wiki/South_Korea'},\n",
       " {'mentionRaw': 'Officials',\n",
       "  'quoteSpeakerCharOffsetsFirst': 974,\n",
       "  'quoteSpeakerCharOffsetsSecond': 983,\n",
       "  'quotation': 'that there could always be changes',\n",
       "  'quoteCharOffsetsFirst': 994,\n",
       "  'quoteCharOffsetsSecond': 1028,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Official',\n",
       "  'mentionLinkLogProb': -0.061121683567762375,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Official'},\n",
       " {'mentionRaw': 'Unknown',\n",
       "  'quoteSpeakerCharOffsetsFirst': -1,\n",
       "  'quoteSpeakerCharOffsetsSecond': -1,\n",
       "  'quotation': 'Trump',\n",
       "  'quoteCharOffsetsFirst': 1034,\n",
       "  'quoteCharOffsetsSecond': 1039,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'Anonymous',\n",
       "  'mention': 'Unknown',\n",
       "  'mentionLinkLogProb': -0.9229084253311157,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Unknown'},\n",
       " {'mentionRaw': 'Unknown',\n",
       "  'quoteSpeakerCharOffsetsFirst': -1,\n",
       "  'quoteSpeakerCharOffsetsSecond': -1,\n",
       "  'quotation': 'to make snap decisions',\n",
       "  'quoteCharOffsetsFirst': 1049,\n",
       "  'quoteCharOffsetsSecond': 1071,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'Anonymous',\n",
       "  'mention': 'Unknown',\n",
       "  'mentionLinkLogProb': -0.9229084253311157,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Unknown'},\n",
       " {'mentionRaw': 'Officials',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1115,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1124,\n",
       "  'quotation': 'internal deliberations',\n",
       "  'quoteCharOffsetsFirst': 1168,\n",
       "  'quoteCharOffsetsSecond': 1190,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Official',\n",
       "  'mentionLinkLogProb': -0.061121683567762375,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Official'},\n",
       " {'mentionRaw': 'Officials',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1115,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1124,\n",
       "  'quotation': 'the',\n",
       "  'quoteCharOffsetsFirst': 1287,\n",
       "  'quoteCharOffsetsSecond': 1290,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Official',\n",
       "  'mentionLinkLogProb': -0.061121683567762375,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Official'},\n",
       " {'mentionRaw': 'Republican leaders on Capitol Hill',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1297,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1331,\n",
       "  'quotation': 'making any hasty exit from Afghanistan that could jeopardize the peace process and undermine counterterrorism efforts',\n",
       "  'quoteCharOffsetsFirst': 1360,\n",
       "  'quoteCharOffsetsSecond': 1477,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Republican Party (United States)',\n",
       "  'mentionLinkLogProb': -0.25988897681236267,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Republican_Party_(United_States)'},\n",
       " {'mentionRaw': 'acting',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1499,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1505,\n",
       "  'quotation': 'US Pentagon chief',\n",
       "  'quoteCharOffsetsFirst': 1506,\n",
       "  'quoteCharOffsetsSecond': 1523,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Acting',\n",
       "  'mentionLinkLogProb': -0.09484714269638062,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Acting'},\n",
       " {'mentionRaw': 'acting',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1499,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1505,\n",
       "  'quotation': 'Afghanistan – Afghans',\n",
       "  'quoteCharOffsetsFirst': 1532,\n",
       "  'quoteCharOffsetsSecond': 1552,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Acting',\n",
       "  'mentionLinkLogProb': -0.09484714269638062,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Acting'},\n",
       " {'mentionRaw': 'acting',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1499,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1505,\n",
       "  'quotation': \"Australian troops'alleged crimes\",\n",
       "  'quoteCharOffsetsFirst': 1571,\n",
       "  'quoteCharOffsetsSecond': 1604,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Acting',\n",
       "  'mentionLinkLogProb': -0.09484714269638062,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Acting'},\n",
       " {'mentionRaw': '##Senate Majority Leader Mitch McConnell',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1665,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1703,\n",
       "  'quotation': 'the Trump administration has made tremendous headway against terrorist threats',\n",
       "  'quoteCharOffsetsFirst': 1709,\n",
       "  'quoteCharOffsetsSecond': 1787,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Mitch McConnell',\n",
       "  'mentionLinkLogProb': -0.07700255513191223,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Mitch_McConnell'},\n",
       " {'mentionRaw': '##Senate Majority Leader Mitch McConnell',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1665,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1703,\n",
       "  'quotation': 'a potentially “ humiliating ” pullout from Afghanistan',\n",
       "  'quoteCharOffsetsFirst': 1808,\n",
       "  'quoteCharOffsetsSecond': 1860,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Mitch McConnell',\n",
       "  'mentionLinkLogProb': -0.07700255513191223,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Mitch_McConnell'},\n",
       " {'mentionRaw': 'he',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1866,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1868,\n",
       "  'quotation': 'that',\n",
       "  'quoteCharOffsetsFirst': 1861,\n",
       "  'quoteCharOffsetsSecond': 1865,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsRightSucceeded',\n",
       "  'mention': 'He (surname)',\n",
       "  'mentionLinkLogProb': -0.3188076317310333,\n",
       "  'links': 'https://en.wikipedia.org/wiki/He_(surname)'},\n",
       " {'mentionRaw': 'he',\n",
       "  'quoteSpeakerCharOffsetsFirst': 1866,\n",
       "  'quoteSpeakerCharOffsetsSecond': 1868,\n",
       "  'quotation': 'said would be worse than President Barack Obama ’ s 2011 withdrawal from Iraq and reminiscent of the U. S. departure from Saigon in 1975',\n",
       "  'quoteCharOffsetsFirst': 1869,\n",
       "  'quoteCharOffsetsSecond': 2002,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'He (surname)',\n",
       "  'mentionLinkLogProb': -0.3188076317310333,\n",
       "  'links': 'https://en.wikipedia.org/wiki/He_(surname)'},\n",
       " {'mentionRaw': 'Rep. Michael McCaul, Republican leader on the House Foreign Affairs Committee,',\n",
       "  'quoteSpeakerCharOffsetsFirst': 2003,\n",
       "  'quoteSpeakerCharOffsetsSecond': 2081,\n",
       "  'quotation': 'of the plans for Afghanistan, “ We need to ensure a residual force is maintained for the foreseeable future to protect U. S. national and homeland security interests and to help secure peace for Afghanistan. ”',\n",
       "  'quoteCharOffsetsFirst': 2087,\n",
       "  'quoteCharOffsetsSecond': 2293,\n",
       "  'SegmentOffset': 0,\n",
       "  'Type': 'TowardsLeftSucceeded',\n",
       "  'mention': 'Michael McCaul',\n",
       "  'mentionLinkLogProb': -0.07016891241073608,\n",
       "  'links': 'https://en.wikipedia.org/wiki/Michael_McCaul'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractText(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8640077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'U. S. officials': ('United States', -0.44718435406684875),\n",
       " 'Trump ’ s': ('Donald Trump', -0.568838894367218),\n",
       " 'who': ('Who (pronoun)', -0.5766157507896423),\n",
       " 'Unknown': ('Unknown', -0.9229084253311157),\n",
       " 'S. officials': ('South Korea', -0.40526676177978516),\n",
       " 'Officials': ('Official', -0.061121683567762375),\n",
       " 'Republican leaders on Capitol Hill': ('Republican Party (United States)',\n",
       "  -0.25988897681236267),\n",
       " 'acting': ('Acting', -0.09484714269638062),\n",
       " '##Senate Majority Leader Mitch McConnell': ('Mitch McConnell',\n",
       "  -0.07700255513191223),\n",
       " 'he': ('He (surname)', -0.3188076317310333),\n",
       " 'Rep. Michael McCaul, Republican leader on the House Foreign Affairs Committee,': ('Michael McCaul',\n",
       "  -0.07016891241073608)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b998acac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genre.entity_linking import get_end_to_end_prefix_allowed_tokens_fn_hf as get_prefix_allowed_tokens_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68e59619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genre.utils import get_entity_spans_hf as get_entity_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcdb529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genre.hf_model import GENRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a1b78e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GENRE.from_pretrained(\"models/hf_wikipage_retrieval\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e4c9f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from genre.trie import Trie\n",
    "with open(\"./models/kilt_titles_trie_dict.pkl\", \"rb\") as f:\n",
    "    trie = Trie.load_from_dict(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14d76d76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'Trump Productions', 'logprob': tensor(-0.7931)}],\n",
       " [{'text': 'Presidential transition of Donald Trump',\n",
       "   'logprob': tensor(-0.9255)}],\n",
       " [{'text': 'Presidential transition of Barack Obama',\n",
       "   'logprob': tensor(-0.9652)}],\n",
       " [{'text': 'Donald Trump 2016 presidential campaign',\n",
       "   'logprob': tensor(-1.0125)}],\n",
       " [{'text': 'Donald Trump Supreme Court candidates',\n",
       "   'logprob': tensor(-1.1937)}]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"President Trump\"]\n",
    "\n",
    "model.sample(\n",
    "    sentences,\n",
    "    prefix_allowed_tokens_fn=lambda batch_id, sent: trie.get(sent.tolist()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1bc4b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_allowed_tokens_fn = get_prefix_allowed_tokens_fn(model, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a659f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'text': 'Trump', 'logprob': tensor(-0.5179)}],\n",
       " [{'text': ' {Trump } [ Donald Trump ]', 'logprob': tensor(-2.1531)}],\n",
       " [{'text': ' {Trump } [ Presidency of Donald Trump ]',\n",
       "   'logprob': tensor(-2.1581)}],\n",
       " [{'text': ' {Trump } [ Donald T. Trump ]', 'logprob': tensor(-2.3450)}],\n",
       " [{'text': ' {Trump } [ Donald Donald Trump ]', 'logprob': tensor(-2.5712)}]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample(\n",
    "    sentences,\n",
    "    prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19285f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacyEntityLinker import EntityLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b2ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp2 = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1c0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "entityLinker = EntityLinker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f150db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.add_pipe(entityLinker, last=True, name=\"entityLinker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7058fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"joe biden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e09c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_linked_entities=doc._.linkedEntities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b9f97f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wikidata.org/wiki/Q6279       6279       Joe Biden                       47th Vice President of the United States (in office from 2009 to 2017)                              \n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    sent._.linkedEntities.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22d08e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "joe biden"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp2(\"joe biden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35fa72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_linked_entities=doc._.linkedEntities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56a953f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.wikidata.org/wiki/Q6279       6279       Joe Biden                       47th Vice President of the United States (in office from 2009 to 2017)                              \n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    sent._.linkedEntities.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a8e588b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-61bbdd048d3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vivek kotecha\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinkedEntities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacyEntityLinker\\EntityCollection.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"vivek kotecha\")\n",
    "str(doc._.linkedEntities[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3557181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tuple(str(i) for i in doc._.linkedEntities[0].get_super_entities(limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a2e27e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'human&billionaire'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'&'.join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df8a54c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 1, 2, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "None,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06029211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertForTokenClassification\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import spacy\n",
    "import neuralcoref\n",
    "from spacyEntityLinker import EntityLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1664399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('albert-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a7d653dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tok\\\\tokenizer_config.json',\n",
       " 'tok\\\\special_tokens_map.json',\n",
       " 'tok\\\\spiece.model',\n",
       " 'tok\\\\added_tokens.json',\n",
       " 'tok\\\\tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84b95479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8d1f8fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "GPU is not accessible. Was the library installed correctly?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-ffa7a905abff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequire_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\neural\\util.py\u001b[0m in \u001b[0;36mrequire_gpu\u001b[1;34m(gpu_id)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mCupyOps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GPU is not accessible. Was the library installed correctly?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCupyOps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCupyOps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: GPU is not accessible. Was the library installed correctly?"
     ]
    }
   ],
   "source": [
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb532d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
